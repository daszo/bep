{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca9474f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "from utils.database import load_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f78c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5f45d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aquire_comparison(table_name: str, name: str):\n",
    "    df = load_db(table_name)\n",
    "\n",
    "    location = \"data/\"\n",
    "\n",
    "    # read form pandas csv\n",
    "    df_bm25 = pd.read_csv(location + \"bm25_\" + name)\n",
    "    print(df_bm25.columns)\n",
    "    df_dsi = pd.read_csv(location + \"dsi_\" + name)\n",
    "    print(df_dsi.columns)\n",
    "    print(df.columns)\n",
    "\n",
    "\n",
    "\n",
    "    # these are the collumn name \n",
    "    # query,target_doc_id,rank,model_predictions\n",
    "    # insert both dataframes on target_doc_id in both datagframes\n",
    "    df_merged = df_bm25.merge(df_dsi, left_on=\"target_text_id\", right_on=\"target_doc_id\", suffixes=('_bm25', '_dsi'), how='inner')\n",
    "\n",
    "    # insert left on test_id in df_csv and elaborative_description in df, ignore if there are more than one matches\n",
    "    df_merged = df_merged.merge(df, left_on=\"target_doc_id\", right_on=\"elaborative_description\", how=\"left\")\n",
    "    # filter rows where rank_bm25 is 1 and rank_dsi is 1\n",
    "\n",
    "    # Force string type on both merge keys\n",
    "    df_merged[\"target_doc_id\"] = df_merged[\"target_doc_id\"].astype(str)\n",
    "    df[\"elaborative_description\"] = df[\"elaborative_description\"].astype(str)\n",
    "    df_rank1 = df_merged[(df_merged['rank_bm25'] == 1) & (df_merged['rank_dsi'] == -1)]\n",
    "    print(f\"Number of rows where BM25 rank is 1 and DSI rank is -1: {len(df_rank1)}\")\n",
    "\n",
    "    df_rank2 = df_merged[(df_merged['rank_bm25'] == -1) & (df_merged['rank_dsi'] == 1)]\n",
    "    print(f\"Number of rows where BM25 rank is -1 and DSI rank is 1: {len(df_rank2)}\") \n",
    "\n",
    "\n",
    "    # save to csv\n",
    "    print(f\"Saving {table_name} to csv...\")\n",
    "\n",
    "    df_rank1.to_csv(location + table_name + \"_bm25_rank1_dsi_rank-1.csv\", index=False)\n",
    "    df_rank2.to_csv(location + table_name + \"_bm25_rank-1_dsi_rank1.csv\", index=False)\n",
    "\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda39bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aquire_comparison(\"N10k\", \"10k_no_thread_results.csv\")\n",
    "aquire_comparison(\"N100k_thread\", \"100k_thread_results.csv\")\n",
    "aquire_comparison(\"N10k_thread\", \"10k_thread_results.csv\")\n",
    "aquire_comparison(\"N10k_thread_same_mid\", \"10k_thread_same_mid_results.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a76043",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_bm25 = pd.read_csv(location + \"bm25_\" + \"10k_thread_same_mid_results.csv\")\n",
    "print(df_bm25.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30937c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# list of tables to aquire\n",
    "tables = [\n",
    "    (\"N10k\", \"10k_no_thread_results.csv\"),\n",
    "    (\"N100k_thread\", \"100k_thread_results.csv\"),\n",
    "    (\"N10k_thread\", \"10k_thread_results.csv\"),\n",
    "    (\"N10k_thread_same_mid\", \"10k_thread_same_mid_results.csv\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1631be01",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load Data\n",
    "file_dsi = \"data/N10k_thread_same_mid_bm25_rank-1_dsi_rank1.csv\"\n",
    "file_bm25 = \"data/N10k_thread_same_mid_bm25_rank1_dsi_rank-1.csv\"\n",
    "\n",
    "df_1 = pd.read_csv(file_dsi)\n",
    "df_2 = pd.read_csv(file_bm25)\n",
    "\n",
    "# 2. Filter NaNs & Select Top 5\n",
    "# We only care if NaNs exist in the columns we are actually going to print\n",
    "cols_to_check = [\n",
    "    'target_text_id', 'query_bm25', 'body', 'body_clean_and_subject',\n",
    "    'doctoquery', 'text_rank_query', \n",
    "    'model_predictions_bm25', 'model_predictions_dsi'\n",
    "]\n",
    "\n",
    "# Drop rows with missing values in these specific columns\n",
    "df_1_clean = df_1.dropna(subset=cols_to_check)\n",
    "df_2_clean = df_2.dropna(subset=cols_to_check)\n",
    "\n",
    "# Now take the top 5\n",
    "df_1_top = df_1_clean.head(5).copy()\n",
    "df_1_top['Group_Label'] = \"DSI Correct / BM25 Incorrect\"\n",
    "\n",
    "df_2_top = df_2_clean.head(5).copy()\n",
    "df_2_top['Group_Label'] = \"BM25 Correct / DSI Incorrect\"\n",
    "\n",
    "df = pd.concat([df_1_top, df_2_top])\n",
    "\n",
    "# 3. Robust LaTeX Escape Function\n",
    "def clean_tex(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    replacements = {\n",
    "        '&': r'\\&', '%': r'\\%', '$': r'\\$', '#': r'\\#', '_': r'\\_',\n",
    "        '{': r'\\{', '}': r'\\}', '~': r'\\textasciitilde{}', '^': r'\\^{}',\n",
    "        '\\\\': r'\\textbackslash{}', '<': r'\\textless{}', '>': r'\\textgreater{}'\n",
    "    }\n",
    "    \n",
    "    pattern = re.compile('|'.join(re.escape(key) for key in replacements.keys()))\n",
    "    safe_text = pattern.sub(lambda x: replacements[x.group()], text)\n",
    "    \n",
    "    # Remove newlines for standard text fields\n",
    "    return safe_text.replace('\\n', ' ').replace('\\r', '')\n",
    "\n",
    "# 4. Helper for Predictions\n",
    "def format_preds(val):\n",
    "    if pd.isna(val):\n",
    "        return \"\"\n",
    "    parts = str(val).split('|')\n",
    "    cleaned = [clean_tex(p.strip()) for p in parts if p.strip()]\n",
    "    return r' \\newline '.join(cleaned)\n",
    "\n",
    "# 5. Helper for Body Text\n",
    "def format_body(val, limit=400):\n",
    "    val_str = str(val)\n",
    "    if len(val_str) > limit:\n",
    "        val_str = val_str[:limit-3] + \"...\"\n",
    "    return clean_tex(val_str)\n",
    "\n",
    "# 6. Generate LaTeX\n",
    "latex_lines = []\n",
    "\n",
    "latex_lines.append(r\"\\begin{longtable}{|p{3.5cm}|p{11.5cm}|}\")\n",
    "latex_lines.append(r\"\\caption{Comparison: Body vs Cleaned Input vs Predictions} \\label{tab:detailed_comparison} \\\\\")\n",
    "latex_lines.append(r\"\\hline\")\n",
    "latex_lines.append(r\"\\textbf{Attribute} & \\textbf{Content} \\\\\")\n",
    "latex_lines.append(r\"\\hline\")\n",
    "latex_lines.append(r\"\\endfirsthead\")\n",
    "latex_lines.append(r\"\\hline\")\n",
    "latex_lines.append(r\"\\textbf{Attribute} & \\textbf{Content} \\\\\")\n",
    "latex_lines.append(r\"\\hline\")\n",
    "latex_lines.append(r\"\\endhead\")\n",
    "\n",
    "current_group = None\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    \n",
    "    if row['Group_Label'] != current_group:\n",
    "        current_group = row['Group_Label']\n",
    "        latex_lines.append(r\"\\multicolumn{2}{|c|}{\\textbf{\\textit{\" + current_group + r\"}}} \\\\\")\n",
    "        latex_lines.append(r\"\\hline\")\n",
    "\n",
    "    target_id = clean_tex(row['target_text_id'])\n",
    "    query = clean_tex(row['query_bm25'])\n",
    "    \n",
    "    body_raw = format_body(row['body'], limit=500)\n",
    "    body_clean = format_body(row['body_clean_and_subject'], limit=500)\n",
    "    \n",
    "    d2q = clean_tex(row['doctoquery'])\n",
    "    tr = clean_tex(row['text_rank_query'])\n",
    "    \n",
    "    preds_bm25 = format_preds(row['model_predictions_bm25'])\n",
    "    preds_dsi = format_preds(row['model_predictions_dsi'])\n",
    "    \n",
    "    # --- ROW BLOCK ---\n",
    "    latex_lines.append(f\"\\\\textbf{{Target ID}} & {target_id} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Query}} & {query} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Body (Raw)}} & {body_raw} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Body (Clean+Subj)}} & {body_clean} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Doc2Query}} & {d2q} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{TextRank}} & {tr} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Preds (BM25)}} & {preds_bm25} \\\\\\\\\")\n",
    "    latex_lines.append(r\"\\hline\")\n",
    "    latex_lines.append(f\"\\\\textbf{{Preds (DSI)}} & {preds_dsi} \\\\\\\\\")\n",
    "    \n",
    "    latex_lines.append(r\"\\hline \\hline\") \n",
    "\n",
    "latex_lines.append(r\"\\end{longtable}\")\n",
    "\n",
    "with open('tbls.tex', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(latex_lines))\n",
    "\n",
    "print(f\"Successfully generated 'tbls.tex'. Filtered {len(df_1)-len(df_1_clean)} rows from DSI and {len(df_2)-len(df_2_clean)} rows from BM25 due to missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc51dc1d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e236f9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3815ec55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
