\documentclass[a4paper, 12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{xcolor}
\usepackage{afterpage}

\usepackage{relsize}
\usepackage{moresize}

\usepackage{graphicx}
\usepackage{geometry}

\usepackage{longtable}
\usepackage{array}
% my additions

\usepackage{subfig}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{cleveref}
% \usepackage[margin=1in]{geometry}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{xurl}
\usepackage{lmodern}

\usepackage[backend=biber, style=numeric, sorting=nty]{biblatex}
\addbibresource{../references.bib}

\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{8000}
\setcounter{biburlnumpenalty}{8000}

\input{../../code/tbls/character_stats.tex}
\input{../../code/tbls/word_stats.tex}
\input{../../code/tbls/general_stats.tex}
\input{../../code/tbls/similarities_stats.tex}
% [CHANGE] The title of your thesis. If your thesis has a subtitle, then this
% should appear right below the main title, in a smaller font.
\newcommand{\theTitle}{Email Content Search: \\
\vspace{0.5em}
A Comparison Between BM25 and Generative Retrieval }
\newcommand{\theSubTitle}{a smaller subtitle}


% [CHANGE] Your full name. In case of multiple names, you can include their
% initials as well, e.g. "Robin G.J. van Achteren".
\newcommand{\theAuthor}{DaniÃ«l S. van Oosteroom}

% [CHANGE] Your student ID, as this has been assigned to you by the UvA
% administration.
\newcommand{\theStudentID}{13354582}

% [CHANGE] The name of your supervisor(s). Include the titles of your supervisor(s),
% as well as the initials for *all* of his/her first names.
\newcommand{\theSupervisor}{Dr. Yubao Tang} % Dr. Ing. L. Dorst

% [CHANGE] The address of the institute at which your supervisor is working.
% Be sure to include (1) institute (is appropriate), (2) faculty (if
% appropriate), (3) organisation name, (4) organisation address (2 lines).
\newcommand{\theInstitute}{
Informatics Institute \\ %Institute for Logic, Language and Computation
Faculty of Science\\
University of Amsterdam\\
Science Park 900 \\ 
1098 XH Amsterdam 
}

% [CHANGE] The semester in which you started your thesis. (Semester 1 or Semester 2, academic year)
\newcommand{\theDate}{Semester 1, 2025-2026}

\begin{document}

\pagestyle{empty}
% Page I

% This page should contain your title and name and will create a thumbnail
% which should be readable at https://scripties.uba.uva.nl/



    \newgeometry{margin=1cm}
    \thispagestyle{empty}
    
    % [CHANGE]
    % You can also use one of the other background colors, 
    % preferably one that fits with your cover-image
    % see https://en.wikibooks.org/wiki/LaTeX/Colors for suggestions
    \pagecolor{black}\afterpage{\nopagecolor}
 
    \begin{minipage}[t][0.8\paperheight]{0.8\paperwidth}
        \begin{center}
        
                  %% Print the title a at the top in white.
                  {\color{white} \fontsize{40}{104}\selectfont \textbf{\theTitle} }
               
                  \vspace{0.2\paperheight}
                  
                  % [CHANGE]
                  % Replace this image with one that is relevant for your research, 
                  % If possible, use one of your own illustrations
                 
                  \includegraphics[width=0.75\paperwidth]{augmented-intelligence}
                  
                   %% Print the author at the bottom 
                  \vspace{0.2\paperheight}
                  
                 {\color{white} \fontsize{24}{48}\selectfont \textbf{\theAuthor} }
        \end{center}
    \end{minipage}   
   
    \restoregeometry
    
\newpage

% Page II

\newgeometry{margin=1cm}
\vspace*{0.8\textheight}
\noindent
Layout: typeset by the author using \LaTeX. \\
Cover illustration: Unknown artist 
\restoregeometry

\newpage

% Page III
\begin{center}

\vspace{2.5cm}


\begin{Huge}
% see definition at beginning of document
\theTitle
\end{Huge} \\

\vspace{0.5 cm}

% \begin{Large}
% \theSubTitle
% \end{Large}

\vspace{1.5cm}

% see definition at beginning of document
\theAuthor\\
% see definition at beginning of document
\theStudentID

\vspace{1.5cm}

% [DO NOT CHANGE]
Bachelor thesis\\
Credits: 18 EC

\vspace{0.5cm}

% [DO NOT CHANGE] The name of the educational programme.
Bachelor \textit{Kunstmatige Intelligentie} \\
\vspace{0.25cm}
\includegraphics[width=0.075\paperwidth]{uva_logo} \\
\vspace{0.1cm}

% [DO NOT CHANGE] The address of the educational programme.
University of Amsterdam\\
Faculty of Science\\
Science Park 900\\
1098 XH Amsterdam

\vspace{2cm}

\emph{Supervisor}\\

% see definition at beginning of document
\theSupervisor

\vspace{0.25cm}

% see definition at beginning of document
\theInstitute

\vspace{1.0cm}

% see definition at beginning of document
\theDate

\end{center}
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}
\pagestyle{plain} 

\abstract{}

\tableofcontents{}

\chapter{Introduction}


Email is to this day a widely used communication medium, yet it consumes a disproportionate time of users day. While surveys often estimate email usage at over three hours per day \cite{Adobe2019}, empirical logging studies reveal that knowledge workers spend approximately 1.5 hours of actual focused time on email daily, checking their inboxes an average of 77 times per day \cite{inproceedings} This is why retrieving relevant past messages is a common and challenging task. The nature of email content is noisy and unstructured, with long threads, quoted replies, inconsistent formatting, and other artifacts that does not contribute to its meaning, making content-based search difficult. We validate our results using the Enron Corpus, which remains the de facto standard benchmark for open-source email analysis.

Currently the most used method is BM25 \cite{INR-019}, which uses term frequency and inverse document frequency to score and rank documents based on query content. BM25 remains a strong baseline in many text retrieval tasks due to its simplicity, interpretability and robust performance, representing strong improvement over prior work. These earlier methods have often relied on lexical matching and indexing approaches, such as the vector space model,for ranking content \cite{strathprints2621}. However, even though words do not have to match exactly, BM25 still suffers from the same semantic gap, which necessitates the presence of the same keywords in the emails as in the search query

Recently, natural language processing (NLP) has seen developments in pretrained language models. Generative retrieval has emerged as a new paradigm that directly generates docuemnt identifiers or representations based on query semantics rather than relying purely on lexical overlap. While generative retrieval shows promise in general IR benchmarks, its performance on noisy, real-world email content has not been systematically evaluated.

\noindent Research questions:
\begin{enumerate}
  \item To what extend can email retrieval performance be imporoved by moving away for lexical matching to a generative approach, and what role does thread history play in bridging the semantic gap?
  \begin{enumerate}
    \item How effective is BM25 for email content retrieval on the Enron corpus?
    \item How does generative retrieval compare with BM25 in this task?
    \item Does email thread context influence retrieval performance across methods?
  \end{enumerate}
\end{enumerate}


In this paper we will construct and email content retrieval benchmark using the enron corpus with both threaded and non-threaded text variants, to identify the impact of conversation history. We first establish a lexical baseline using BM25, followed by the implementation and evaluation of a generative retrieval method. This study will conclude with a comparative analysis, providing qualitative results and qualitative insights into the retrieval behaviors and performance disparities between the generative and lexical approaches. 
%
% A perpetual problem of searching through emails is the semantic mismatch problem. This causes a lot of searching time when users search through their emails. Currently the most used searching method is BM25 \cite{INR-019} and it searches using word matching. This is not sufficient as it still needs a query to be quite similar to the contents of the documents. It would be better if users could ask question and the meaning of such questions would be used for searching. An improvement on this is dense vector retrieval \cite{karpukhin-etal-2020-dense}. This turns text into a one dimensional vector and calculates the similarities between the query and the documents. This works better as users may now ask question in language that comes natural to them, but has the problem of not being end to end differential. To solve this \textcite{NEURIPS2022_892840a6} proposed a generative method called the differentable search index (DSI) turning indexing and task training into one optimizable step. 
%
% Generative retrieval (GR) is still a novel field and DSI has been mainly used for curated datasets or books, thus it is interesting to explore what else DSI could be used for. To see how well DSI performs on an email dataset, the Enron corpus could be used \cite{klimt2004introducing}. In this research, we explore to what extent DSI can be used to search on the Enron corpus and how well does it compare to BM25. 

\chapter{Related Research}


In this chapter just enough overview of the current state of information retrieval is given.Here there will be explained how IR moved from exact word matching to end to end optimizing a search engine. 

\section{Traditional Information Retrieval}

Some of the traditional techniques are sometimes still used to day. These techniques are characterized by their heaving emphasis on lexical matching. What unites these techniques is the use of tokenization and normalization. Tokenization is the act of splitting up text into a set unit. A common way for these techniques to tokenize it to break down a text into words. Part of Normalization is making all words lowercase. Stemming is also part of this practice and involves casting words to their simplest for i.e. running to run, and stop words removal.

\subsection{Inverse Index}

Inverted Index \cite{strathprints2621} is a method that creates a dictionary that saves for every term the documents it is apart of. This is done to circumvent the problem of sparse vectors and thus save memory. To search for a document, the tokens from the query where input into this dictionary and the intersection of all the documents that contained the tokens was retrieved. 

\subsection{BM25}
BM25 is a sparse retrieval ranking function [source] that is the industry standard baseline for retrieval tasks. It excels well on lexical matching, which is also it's weakness . The order of words is not important as it uses the bag of words theorem. 

The formula for BM25 \ref{eq:bm25_score} takes query tokens of $Q = {q_1, q_2, ..., q_i}$ to score a given document D.

\begin{equation}
\label{eq:bm25_score}
\text{score}(D, Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i, D) \cdot (k_1 + 1)}{f(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}
\end{equation}

This method is made up of 3 parts. The first part is the term frequency, $f(q_i, D)$. This is the amount of times that that a keyword $q_i$ occurs in the given document. The second parts is the document length normalization, $\frac{|D|}{\text{avgdl}}$. This function of this is penalizing large documents, which otherwise would be dominant. The last part is the inverted document frequency (IDF) \ref{eq:IDF}. This acts as a weight that rewards rarity by calculating how much it occurs in the entire corpus. 
\begin{equation}
\label{eq:IDF}
\text{IDF}(q_i) = \ln\left( \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5} + 1 \right)
\end{equation}

The variables k an b are hyper parameters and control the weight of the term frequency and the length normalization respectively


\section{Transformer architecture}
The methods discussed in the previous section both suffer form the semantic gap. Which is a problem where the query uses a differnt lexical form to the documents itself. The transformer architecture [cite] aims to solve this problem. This is a method that utilizes position of words and derives meaning form this. Tranfomers, are a deep neural network and cast words into ins latent space, which solves a large part of the semantic gap. The latent space is a term used in deep learning and refers to the hidden parameters that reside inside of the model and is of a different form then text. Transformer architecture excels at sequence to sequence tasks, which are tasks where the input and the output are sequences. [bert]

These there two types of NLP tasks, holistic and tokenized ones [bert]. The holistic type operates with text on the sentence level. The tokenized answers questions and attribution of entities.


\subsection{BERT}
Bert is a deep bi-directional and holistic linguistic model. It uses masked language modeling to train the model. This is where random words are selected and masked and predicted by the model, taking only into account they surrounding context. Bert allows for fine tuning.

\subsection{Dense Retrieval}


Dense retrieval method  \cite{karpukhin-etal-2020-dense} replaces sparse retrieval methods like BM25 with a dense embedding based system for open domain question answering. This method uses a tokenizer and dual-encoder frame work which encodes queries into a dense vector representation. The similarity between two passages is calculated by taking the dot product between their  respective vectors \ref{eq:dense_retrieval}.

\begin{equation}
\text{sim}(q, p) = E_Q(q)^\top E_P(p)
\label{eq:dense_retrieval}
\end{equation}


\section{Generative Retrieval}

In the previous sections the methods described are what is sometimes called retrieve-then-rank strategies [cite dsi]. On the other hand, generative retrieval integrates the indexing and retrieval into one model. This allows for end to end optimization as the whole task is one differentiable model. It's output is the document identifier (docid) and learns the connection between the documents and the docids.


\subsection{DSI}

Differentiable search index is a finetuned transformer [source] which encodes all the trained information of the corpus withing the parameters of the model itself. Beam first search may be used to produce a ranked list of potentially-relevant docids. The indexing and training of the task are done at the same time which allows for end to end optimization. The unique identifiers can be atomic intergers, strings integers or semantically structured string. The semantically structured strings are generated by hierachical clustering and giving names depeding on the cluster the documents are in. 



\subsection{SE-DSI}

% A problem that arises with DSI is that there is a difference in data distribution \cite{2022arXiv220610128Z}. The indexing data and the docids are of different length. To combat this problem, pseudo-queries are generated of the original text and used instead. 
SE-DSI stands for semantic-enhanced differentiable search index \cite{10.1145/3580305.3599903} and improves over the DSI method by including rehearsal steps and replacing the docids with a unique strings derived from the contents of the document call the elaborative description. Now that the input and output the model can use it's understanding of words to map queries better to the docids.


% TODO: this is still a little bare. The individual techniques should have a bit more depth.

%
% The first big breakthrough on this was the transformer model \cite{NIPS2017_3f5ee243}. This is a version of a recurrent neural network which takes sentences and attributes weights to the position of the words. This is called attention and allows search on meaning and not exact word matching. These models are also called large language models (LLM). A common used case for LLM's is question and answering, which is very useful for retrieval as it allowed users to specify their question in the users natural language [to intoduction]. The big problem with these types of LLM's is that they hallucinate and are not faithful to sources. Dense vector retrieval solves this issue because it turns text into a one dimensional vector and calculates the similarities between the query and the documents. Although Dense vector retrieval can also use the transformer architecture it stays faitfull to the sources, by only retrieving the relevant doucments and not augmenting them. 
%
% The problem in dense vector retrieval is that the indexing and retieval are two seperate tasks. [tay et all] introduced a method where the indexing and the retrieval are trained at the same time, allowing for end to end optimalization. This is called the differentiable search index (DSI) which is part of the generative retrieval domain (GR). SE-DSI [reference y.tang] improves on this as it takes docids and makes them semantic and similar to the index documents it refers to. Using the geometric shape for the input as the output and constraining the retrieval using a tree allows the model to find relevant documents more easily. Even when the model is wrong, the given output is likely to be similar. Because of this, SE-DSI seems to be the most likely fit for our task.


\section{The Enron Corpus}

Enron was an oil and gas company that  was under investigation of the FBI \cite{BritannicaEnron, FBIEnronCase} after it filed for bankruptcy in 2001. Subsequently, during the investigation, the FBI made the emails of mostly Enron's high management avaiable. Later the dataset was bought by MIT and cleaned, because it contained integrity problems. It has been used to study network relations [source]. Also it has been used widely in spam detection. The original version of the dataset is not available anymore as it contained a lot of privacy sensative information. The dataset was re released by mit and in 2016 and is the dataset that will be used in this paper.

% Email datasets have widely been used for the training of spam detection algorithms [find reference] and for studying network relations [find reference]. One of these datasets is the Enron dataset \cite{klimt2004introducing,klimt2004enron, enron_dataset}. Enron was energy company that specialized in oil and gas and filed for bankruptcy in 2001 \cite{BritannicaEnron, FBIEnronCase}. During the investigation this dataset was made public and a cleaned version is publicly available \cite{enron_dataset}. 


% TODO: add more citations
\chapter{Methods}

\section{Task Definition}
\label{subsec:task-definition}

Let $\mathcal{D} = \{d_1, d_2, \dots d_N \}$ denote the corpus of N email documents, which consists of subject, sender, recipient and sender. Let $\mathcal{Q} = \{q_1, q_2, \dots q_M \}$ represent the set of M queries. Further, we define $\mathcal{Y} = \{y_1, y_2, \dots y_N \}$ as the set of unique document identifiers corresponding to each email document. \\
Let $f_\theta$ be the GR model parameterized by $\theta$. The task of the model is to map query $q \in \mathcal{Q}$ or $d \in \mathcal{D}$ to output relevant documents identifier $y \in \mathcal{Y}$.

To do this, a training set is construction $\mathcal{T}$, this is defined as the union of the document-id pair and query-id pair~\eqref{eq:training-set}.
\begin{equation}
  \mathcal{T} = \{(d,y_d) | d \in \mathcal{D} \} \cup \{(q, y_q) | q \in \mathcal{Q}\}
  \label{eq:training-set}
\end{equation}
Where $y_d$ is the unique identifier assigned to document $d \in \mathcal{D}$ and $y_q$ is the unique identifier assigned to query $q \in \mathcal{Q}$.

The GR retrieval model, $f_\theta$ follows an encoder-decoder architecture. The encoder encodes the query $q$ or the email document $d$ and generates the unique document identifier $y$.

\section{Dataset}
The number of emails present after dropping automatically generated folders is \NoFolderLength. This include the inbox folders and sent / sent\_items / outbox. This is a similar number to the cleaned dataset found in \citeauthor{klimt2004enron}. They found a value of 200,399 documents after cleaning. Their different number of selected emails may have been due to their objective to classify the folders that each email belonged to, for which reason they sought only folders created by humans. After cleaning further the number of remaining emails left is \GeneralLength.

[how was the data intially loaded?]
To use the email bodies, it is important to clean them to reduce the real world noise. Case studies on random samples were conducted to construct regex patterns to clean the data. Headers, emails, websites, phone numbers, signature footers, legal disclaimers and email provider artifacts were removed this way. Email history threads were split and cleaned individually. For the analysis in this section the history threads are kept.  Preliminary experiments showed that email header data creates noise, thus were removed. We employed input lionization by concatenating the subject and body, utilizing field identifiers. This also created noise, though this was kept for some of the datasets because of an error and time constraints, more detail on this in section \ref{subsec:traingin-set-construction}. For a detailed case study showing the effects email header data and including field identifiers, see Appendix \ref{app:schema_impact}. Following earlier work on cleaning data \cite{JMLR:v21:20-074}, the emails smaller then 30 words are removed. Also they would be too  small to create queries and docids of.

\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/character_count_kde.png}
        \label{fig:chr_count_kde}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/word_count_kde.png}
        \label{fig:wrd_count_kde}
    }%
    \caption{KDE plot}
    \label{fig:main_kde_plots}
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/character_counts_boxplot.png}
        \label{fig:chr_box_plot}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/word_counts_boxplot.png}
        \label{fig:wrd_box_plot}
    }%
    \caption{Box plot}
    \label{fig:main_box_plots}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.8\textwidth]{../../code/plots/similarities_hist.png}
  \caption{Distribution of similarities}
  \label{fig:similarities}
\end{figure*}


For the word count the Mean \wordMean{} as shown in \ref{fig:wrd_count_kde} and \ref{fig:wrd_box_plot} is substantially larger than the Median \wordMedian{} which indicates that this is not a normal distribution. This becomes clearer when comparing the Q3 \wordQthree{} to the Max value \wordMax{}. The substantial gap explains the size of the mean. The same is applicable to the character count.

To determine if the subjects of the emails have any similarity to their respective bodies a sentence transformer [source sentence transformer] has been used to calculate the embedding and the cosine similarity has be computed. In the process of calculating the cosine similarity [source cosine sim] the dot product of the subject and body embeddings is taken and the distribution is plotted in figure \ref{fig:similarities}. Although the distribution is unimodal, it is not a perfect bell curve. This is also apparent when looking at the Skewness which is a negative skew of \similaritiesSkewness making it clear that the subjects and the bodies are very similar.

\section{Generating labels}
To train the model without manual labels, we constructed training data solely on the email content and email subject itself. With this process we create the docids and pseudo queries.



\subsection{Email ID construction}
Identifiers should capture the essence of the mail while remaining compact. Following \textcite{10.1145/3690624.3709435}, we use text rank to extract key words of interest from the emails bodies. These keywords will server as docids. Text rank is a graph based ranking system that scores text units, like words or sentences, based on their co-occurence or similarity respectively and selects the best text units using Page Rank \cite{ilprints422}.

A problem that arose is that text rank generates docids that are not always unique. It is standard to ignore this issue \cite{10.1145/3580305.3599903}, because duplicate docids are for emails that are similar in contents anyway.

% TODO:[cite package sumsum]

\subsection{Pseudo-query construction}
When constructing queries it is important that the original words are contained but also cover the essence of the email. Which is why we use an extractve method and an abstractive method in constructing the pseudo-queries. Textrank was employed to extract the most important sentence as pseudo-query. For the abstractive query construction method the t5-headline transformer model \cite{pleban2020t5headline} was used for summarizing the email bodies. When textrank was empty the mail was truncated to 15 words and used instead. [How is textrank empty]

% TODO:[cite package sumsum]

\subsection{Training set construction}
\label{subsec:traingin-set-construction}

For the history thread dataset \cite{10.1145/3580305.3599903}, a 10k and a 100k subset is used, because and that this size is a reasonable scale for T5 to learn from. For the dataset without history thread only a 10k dataset was used as after dropping email smaller then 30 only 93k emails were left. Per data point 3 data pairs are created. These pairs consist of the Semantic Document Identifier paired with one of the following: email body and subject, the extractive query or the abstractive query \ref{eq:training-set}.

As briefly covered in [ref dataset methods] it was planned to remove the field identifier when applying input linerization for the title generation. Due to an error it only applied to the 10k history thread dataset. See appendix \ref{app:schema_impact} for a more in depth table.




\section{Model training}
 % \cite{NEURIPS2022_892840a6} and your improvement \cite{10.1145/3580305.3599903} or can I just quote that?
As specified in \cref{subsec:task-definition} $f_\theta$ is a GR model with the encoder decoder architecture. For this the T5 model was chosen because that is the standard used in previous works \cite{NEURIPS2022_892840a6}\cite{10.1145/3580305.3599903}. The training of the model is a joint mode of two parts i.e., indexing and the retrieval in an end to end way.
\textbf{Indexing:} This step is for memorizing information about the email documents, document are mapped to an docid \cite{NEURIPS2022_892840a6}.

\begin{equation}
  \mathcal{L}_{Indexing}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:indexing-los}
\end{equation}

% While \textcite{NEURIPS2022_892840a6} use only numerical ids \textcite{10.1145/3580305.3599903} use Elaborative Descriptions as the identifier. 

\textbf{Retrieval:} Given a query $q \in \mathcal{Q}$, the DSI model outputs the relevant document identifier $y$. For this the T5 model, previously fine tuned for indexing, is trained on the query-id pair.

\begin{equation}
  \mathcal{L}_{Retrieval}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:retrieval-loss}
\end{equation}

For training the model, \cite{10.1145/3580305.3599903,NEURIPS2022_892840a6} a top 10 greedy beam first search is used to help the model find a valid document identifier. This works because a trie has been build using the docids and guides the model to the right answers. For every token it generates, it predicts for every possible token the probability that it is that token. From that list, only the tokens that are in the trie are selected and the 10 highest probability are kept. For every consecutive token the current probability is summed with the new token. At some point the trie ends and then the 10 most probable are ranked accordingly and scored.

For training, the data set queries were split up in three groups. The first 80\% was reserved for training and then 10\% for both validating and testing. It was made sure that the queries of the validation and testings set were not of the same email. This was done to ensure that the model has seen at least one example of the retrieval task. The email bodies were exclusively reserved for training, because they serve only for learning the model what each unique document identifier means. 

\chapter{Experimental Setup}

In this chapter the setup of the experiment is covered. Upon the methods in the previous chapter is expanded.

\section{Baseline Methods}

As a baseline method only BM25 is used with the pyserini package \cite{DBLP:conf/sigir/LinMLYPN21}. Because our dataset is simlar to the MS MARCO dataset used in the pyserini paper, those optimized hyperparameters are use. Those are k1 = 0.9 and b = 0.4.

\section{Evaluation metrics}
For the evaluation metric, the same metrics were used as in \textcite{NEURIPS2022_892840a6} and \textcite{10.1145/3580305.3599903}. These are mean reciprocal rank (MRR) for 3 and 20 and hits at 1 and 10.

MRR is calculated as the mean of the reciprocal rank \ref{eq:mrr}, the latter is the first position of a relavant document.

Hits at k is calculated \ref{eq:hits} by taking the mean of all the ranks up to k of relevant documents.


\begin{equation}
  \text{MRR}  = \frac{1}{N} \sum_{i=1}^{N} \text{RR}_i
  \label{eq:mrr}
\end{equation}


\begin{equation}
\text{Hits}@k = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I}(\text{rank}_i \le k)
  \label{eq:hits}
\end{equation}

\section{Implementation details}
The model was trained similar to \textcite{10.1145/3580305.3599903} with some adjustments made based on what was provided with the code used \cite{zhuang2022dsiqg}. We employed a Transformer-based encoder-decoder architecture for our DSI task, initializing the model with the pre-trained T5-base \cite{JMLR:v21:20-074}. This architecture comprises a hidden size of 768, 12 Transformer layers, and 12 self-attention heads. We use the Adam optimizer with a learning rate of $5e-4$ and a linear warm-up over 100,000 steps. The model is trained for a maximum of 100,000 steps with a global batch size of 64 (32 per device) on two GPUs. The maximum sequence length is set to 32. Right before training the emails are truncated to x tokens as the model does not allow more. 

\section{Code}
For training of the model the code of \textcite{zhuang2022dsiqg} was used as a basis. It did need to be adapted to work with the sqlite database used in this research. This code however was made for numerical tokens and not semantic tokens. To solve this problem the code to build the trie was used from \textcite{LI2023103475}.

\chapter{Experimental Analyses}

\input{../../code/tbls/table_combined.tex}
\input{../../code/tbls/table_ratios.tex}

The comparison of the baseline with the DSI and impact of data cleaning or data size is shown in  table \ref{tab:combined_results}. This table is split into two tables where the methods used are in the left column. In the top table the thread settings are compared and in the bottom the training size is compared. Besides the methods column, the tables are split into two main columns, corresponding to a dataset, which are under divided into the 4 evaluation metrics, MRR@3, MRR@20, Hits@1 and Hits@10. The ratio between between DSI's performance and BM25 performance is shown in table \ref{tab:ratio_results}

\textbf{Performance baseline BM25:} BM25 performed best without context. The reason for this might be because the No Context dataset does not include the emails history, making the emails small, which is when BM25 performs best. Comparing 10k Context and 100k Context the performance drops a little which is similar previous findings \cite{10.1145/3580305.3599903} \cite{NEURIPS2022_892840a6}.

\textbf{Performance DSI:} DSI performed best when there was no Context. This could be because the email context could have more topics, which might confuse the model. The difference between Context and no context is not too large, which might be because of the automatic truncation. DSI performs worse on bigger datasets. This is not inline with the findings of \textcite{10.1145/3580305.3599903}, but is in line with \textcite{NEURIPS2022_892840a6}.


\textbf{Comparison BM25 and DSI} Compared to BM25, DSI performs worse on all metrics. This stands in Contrast to \textcite{10.1145/3580305.3599903} and \textcite{NEURIPS2022_892840a6}, who reported significant improvement over BM25. A reason for the poor performance might be because this research uses only 2 training queries compared to 10 \cite{10.1145/3580305.3599903}. Also the enron dataset is a real world dataset and might introduce a lot more noise then curated Q\&A datasets. Further, the data might be too complex for binary relevance of our MLE method.

% Also, when cleaning the 10k No Context dataset, less emails were bigger then 93.000

\textbf{Thread settings comparison:} Although the thread dataset did not contain the field identifiers there is a slight better performance between both BM25 and DSI when there is no context on context. On the one hand this is surprising because by including the field identifiers, more noise would be present in generating the title generated pseudo queries. On the other hand this could be explained by the context dataset containing a lot of duplicate email, because the email history is of email addresses also contained in the dataset. When inspecting the ratios context performed a bit better, which could be because the training data contained more words, thus providing more data for the DSI model to learn from or because no field identifiers were present.


\textbf{Scale comparison:} As shown in \textcite{10.1145/3580305.3599903} and \textcite{NEURIPS2022_892840a6}, we would expect DSI to improve from the 10.000 dataset to the 100.000 dataset. This however is not the case. DSI perfoms significantly worse. 



\chapter{Discussion}

On the current version of the data set, BM25 remains a strong baseline for the Enron corpus, particularly on shorter documents. 
DSI underperformed significantly compared across all metrics. Which stands in stark contrast to the trends in recent literature [cite], where DSI outperforms sparce retrieval. 
Including thread history did not significalntly bridge the semantic gap and in the case of the 100k dataset, degraded the performance or introduced noise.

In previous works DSI was used on Natural Questions or MS MARCO, which are clean encyclopedic texts. Enron is a real world dataset, whcih means that it is noisy and unstructured. This makes it harder to sematically map between the pseudo-queries and the docids. 
An other cause for DSI poor performance could be that this research uses the most direct MLE objective, which models relevance as a binary signal. This dichotomy, takes relevance as binary which might not be representative of real world data contained in the Enron dataset. This might confuse the model by "near-duplicates" in the training data.
Further, due to resource limitations the amount of queries generated for training was 2, although earlier works used 10+ [cite], which is less then other works. The datasets that included the field identifiers art enacting for generating the doctoqueries, might have learning to mach noise instead of semantics.

Because there are a lot of similar docids, memorizing the mapping within the model weights becomes more difficult. This could also explain why the performance of the context dataset performed worse, because the history is shared among multiple emails, the docids might have become more similar. 
Further explanation why the thread was outperformed, might be because of topic drift. The semantic docids, might not have been able to capture the shift form i.e. "meeting on Tuesday" to "budget approval" later in the tread. Also distinct parts of emails might be at the end of the emails or thread losing unique input. 
Regex cleaning is brittle on natural data. Not all cleaning was successful \ref{app:compare} and artifacting remains. Noise effects neural models disproportionately because they try to learn patterns, whereas BM25 ignorance /noise more easily. 
In the case the context the comparison is not a true comparison, because they don't have consistent field identifiers. 

Because artifacting persisted a more robust way of cleaning is necessary to continue the use of the dataset for this task. To continue the exploration of this dataset with this task, a more robust cleaning strategy is necessary to combat the persisting artifacting . Because the artifacting is caused by real world data, it is inconsistent and it could be interesting to perform the cleaning by Large Language model.

Rehearsal content could be added, especially for the context dataset, to increase performance.
For DSI the thread and no context performance was very close, which could because by truncation. To include the important information of the context [tang et all book] an extraction of important information could be done.

The inclusion of context has the effect that there are a many duplicates of emails, making a lot of emails similar. A score could be given for emails that are contained in the context and are retrieved. Similarly a score could be given to semantically similar docids or index text, move away form the binary task.

Applying the current method on a curated Q\&A like Natural Questions or the MS MARCO dataset. This could give insight into the efficacy of the used methods. Further, to check if the data has potential, a simpler task could used that works better as a dichotomy. A task like spam detection for instance, which has been done before and was its original use. [reference to spam detection on the Enron dataset].

To generate the doc to queries the model was instructed by including "subject:" or "body:" followed by the subject and body respectively. This cluttered the queries as the doctoquery model was not made to accept instructions. This had the effect that the words subject and body where included often in the queries \ref{app:schema_impact}, because the model might be sensitive to structural patterns. Thus inducing noise. In further work this could be omitted. 

Currently the Context data set uses the whole context for generating the docids and queries. This might induce clutter and might result in very similar docids and queries.

The context dataset might have had to many topics, to verify a study could be conducted modeling the topics using LDA or calculating the similarity using dense vector embeddings.

An other option for Generative retrieval on the Enron corpus is constructing a model that natively takes in the relationship between emails and users. 

It would also be interesting to see how other baselines perform on the dataset and what the performance increase could be, by optimizing the parameters of BM25

[refer to the appendix]


\chapter{Conclusion}

This study investigated the efficacy of Generative Retrieval (GR) on the Enron email corpus. It aimed to bridge the semantic gap inherent to lexical models (BM25), by using differential search index (DSI). It did so by creating a solid baseline and varying between history thread and dataset size.

BM25 proved to still be a strong baseline and was a competitive for the Enron corpus. It consistently outperformed DSI, across dataset size and thread context. Suggesting that BM25 is still a strong baseline despite the semantic limitations of lexical matching. 

The DSI model proved highly sensitive to noise and failed to outperform the baseline. Indicating that ...

In this paper we show that it is not conclusive if the DSI is a good fit for searching email datasets like the Enron corpus. Cleaning has an effect on the performance, but the main impact comes for the method used. BM25 is clear to be better method for this task and we propose investigating adaptions to the DSI method.



{\raggedright
\printbibliography
}

\appendix
\chapter{Impact of Schema Artifacts on Model Performance}
\label{app:schema_impact}

\input{../tables/email_noise}

\chapter{Comparison between BM25 and DSI}
\label{app:compare}
{\raggedright
\input{../../code/tbls.tex}
}
\end{document}
