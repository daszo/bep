\chapter{Methods}

\section{Task Definition}
\label{subsec:task-definition}

Let $\mathcal{D} = \{d_1, d_2, \dots d_N \}$ denote the corpus of N email documents, which consists of subject, sender, recipient and sender. Let $\mathcal{Q} = \{q_1, q_2, \dots q_M \}$ represent the set of M queries. Further, we define $\mathcal{Y} = \{y_1, y_2, \dots y_N \}$ as the set of unique document identifiers corresponding to each email document. \\
Let $f_\theta$ be the GR model parameterized by $\theta$. The task of the model is to map query $q \in \mathcal{Q}$ or $d \in \mathcal{D}$ to output relevant documents identifier $y \in \mathcal{Y}$.

To do this, a training set is construction $\mathcal{T}$, this is defined as the union of the document-id pair and query-id pair~\eqref{eq:training-set}.
\begin{equation}
  \mathcal{T} = \{(d,y_d) | d \in \mathcal{D} \} \cup \{(q, y_q) | q \in \mathcal{Q}\}
  \label{eq:training-set}
\end{equation}
Where $y_d$ is the unique identifier assigned to document $d \in \mathcal{D}$ and $y_q$ is the unique identifier assigned to query $q \in \mathcal{Q}$.

The GR retrieval model, $f_\theta$ follows an encoder-decoder architecture. The encoder encodes the query $q$ or the email document $d$ and generates the unique document identifier $y$.

\section{Corpus Analysis}
\label{section:corpus-an}


\paragraph{Corpus attributes}
The number of emails present after dropping automatically generated folders is \NoFolderLength. This include the inbox folders and sent / sent\_items / outbox. This is a similar number to the cleaned dataset found in~\citeauthor{klimt2004enron}~\cite{klimt2004enron}. They found a value of 200,399 documents after cleaning. Their different number of selected emails may have been due to their objective to classify the folders that each email belonged to, for which reason they sought only folders created by humans. After cleaning further the number of remaining emails left is \GeneralLength.

To use the email bodies, it is important to clean them to reduce the real world noise. Case studies on random samples were conducted to construct regex patterns to clean the data. Headers, emails, websites, phone numbers, signature footers, legal disclaimers and email provider artifacts were removed this way. Email history threads were split and cleaned individually. For the analysis in this section the history threads are kept.  Preliminary experiments showed that email header data creates noise, thus were removed. We employed input lionization by concatenating the subject and body, utilizing field identifiers. This also created noise, though this was kept for some of the datasets because of an error and time constraints, more detail on this in section \ref{subsec:traingin-set-construction}. For a detailed case study showing the effects email header data and including field identifiers, see Appendix \ref{app:schema_impact}. Following earlier work on cleaning data~\cite{JMLR:v21:20-074}, the emails smaller then 30 words are removed. Also they would be too  small to create queries and docids of.

\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/character_count_kde.png}
        \label{fig:chr_count_kde}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/word_count_kde.png}
        \label{fig:wrd_count_kde}
    }%
    \caption{KDE plot}
    \label{fig:main_kde_plots}
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/character_counts_boxplot.png}
        \label{fig:chr_box_plot}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.45\textwidth]{../../code/plots/word_counts_boxplot.png}
        \label{fig:wrd_box_plot}
    }%
    \caption{Box plot}
    \label{fig:main_box_plots}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.8\textwidth]{../../code/plots/similarities_hist.png}
  \caption{Distribution of similarities}
  \label{fig:similarities}
\end{figure*}


For the word count the Mean \wordMean{} as shown in~\ref{fig:wrd_count_kde} and~\ref{fig:wrd_box_plot} is substantially larger than the Median \wordMedian{} which indicates that this is not a normal distribution. This becomes clearer when comparing the Q3 \wordQthree{} to the Max value \wordMax{}. The substantial gap explains the size of the mean. The same is applicable to the character count.

To determine if the subjects of the emails have any similarity to their respective bodies a sentence transformer~\cite{ni-etal-2022-sentence} has been used to calculate the embedding and the cosine similarity has be computed. In the process of calculating the cosine similarity~\eqref{eq:cosine_similarity} the dot product of the subject and body embeddings is taken and the distribution is plotted in figure~\ref{fig:similarities}. Although the distribution is unimodal, it is not a perfect bell curve. This is also apparent when looking at the Skewness which is a negative skew of \similaritiesSkewness \ making it clear that the subjects and the bodies are very similar.
\begin{equation}
    \text{similarity}(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
    \label{eq:cosine_similarity}
\end{equation}
\paragraph{Training data construction}
To train the model without manual labels, we constructed training data solely on the email content and email subject itself. With this process we create the docids and pseudo queries.


\begin{itemize}[leftmargin=*,nosep]
\item \textbf{Email ID construction}
Identifiers should capture the essence of the mail while remaining compact. Following~\textcite{10.1145/3690624.3709435}, we use text rank to extract key words of interest from the emails bodies. These keywords will server as docids. Text rank is a graph based ranking system that scores text units, like words or sentences, based on their co-occurence or similarity respectively and selects the best text units using Page Rank~\cite{ilprints422}. A problem that arose is that text rank generates docids that are not always unique. It is standard to ignore this issue~\cite{10.1145/3580305.3599903}, because duplicate docids are for emails that are similar in contents anyway.


\item \textbf{Pseudo-query construction}
When constructing queries it is important that the original words are contained but also cover the essence of the email. Which is why we use an extractve method and an abstractive method in constructing the pseudo-queries. Textrank was employed to extract the most important sentence as pseudo-query. For the abstractive query construction method an LLM was used for summarizing the email bodies. When textrank was empty the mail was truncated to 15 words and used instead. 
\end{itemize}






\paragraph{Training set variants}
\label{subsec:traingin-set-construction}

For the history thread dataset, a 10k and a 100k subset is used~\cite{10.1145/3580305.3599903}, because this size is a reasonable scale for T5 to learn from. For the dataset without history thread only a 10k dataset was used, because after dropping email smaller then 30 only 93k emails were left. Per data point 3 data pairs are created. These pairs consist of the Semantic Document Identifier paired with one of the following: email body and subject, the extractive query or the abstractive query \eqref{eq:training-set}.

As briefly covered in~\cref{section:corpus-an} it was planned to remove the field identifier when applying input linerization for the title generation. Due to an error it only applied to the 10k history thread dataset. See appendix \ref{app:schema_impact} for a more in depth table.




\section{Model training}
 % \cite{NEURIPS2022_892840a6} and your improvement \cite{10.1145/3580305.3599903} or can I just quote that?
As specified in \cref{subsec:task-definition} $f_\theta$ is a GR model with the encoder decoder architecture. For this the T5 model was chosen because that is the standard used in previous works~\cite{NEURIPS2022_892840a6, 10.1145/3580305.3599903}. The training of the model is a joint mode of two parts i.e., indexing and the retrieval in an end to end way.

\textbf{Indexing:} This step is for memorizing information about the email documents, document are mapped to a docid~\cite{NEURIPS2022_892840a6}.

\begin{equation}
  \mathcal{L}_{Indexing}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:indexing-los}
\end{equation}

% While \textcite{NEURIPS2022_892840a6} use only numerical ids \textcite{10.1145/3580305.3599903} use Elaborative Descriptions as the identifier. 

\textbf{Retrieval:} Given a query $q \in \mathcal{Q}$, the DSI model outputs the relevant document identifier $y$. For this the T5 model, previously fine tuned for indexing, is trained on the query-id pair.

\begin{equation}
  \mathcal{L}_{Retrieval}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:retrieval-loss}
\end{equation}

For training the model, a top 10 greedy beam first search \cite{10.1145/3580305.3599903,NEURIPS2022_892840a6} is used to help the model find a valid document identifier. This works because a trie has been build using the docids and guides the model to the right answers. For every token it generates, it predicts for every possible token the probability that it is that token. From that list, only the tokens that are in the trie are selected and the 10 highest probability are kept. For every consecutive token the current probability is summed with the new token. At some point the trie ends and then the 10 most probable are ranked accordingly and scored.

For training, the data set queries were split up in three groups. The first 80\% was reserved for training and then 10\% for both validating and testing. It was made sure that the queries of the validation and testings set were not of the same email. This was done to ensure that the model has seen at least one example of the retrieval task. The email bodies were exclusively reserved for training, because they serve only for learning the model what each unique document identifier means. 
