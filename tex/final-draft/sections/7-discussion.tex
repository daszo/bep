\chapter{Discussion}

Our experimental results show that BM25 remains a strong baseline for email content retrieval on the Enron corpus, particularly for shorter and less noisy documents. Across all evaluation metrics and dataset configurations, BM25 consistently outperformed DSI. This finding contrasts with recent studies \cite{10.1145/3580305.3599903, NEURIPS2022_892840a6}, where DSI was shown to outperform sparse retrieval methods on cleaner and more curated datasets.

\paragraph{Impact of Noise and Data Characteristics}
A key factor behind this discrepancy is the nature of the Enron corpus. Unlike datasets such as Natural Questions or MS MARCO, which consist of relatively clean and encyclopedic text, Enron emails are noisy, unstructured, and contain artifacts such as quoted replies, signatures, and formatting irregularities. These characteristics make it difficult for DSI to learn stable mappings between pseudo-queries and document identifiers. Although regex-based preprocessing was applied, substantial artifacting remained, which disproportionately affected neural generative models that attempt to learn patterns from data, while BM25 was more robust to such noise.

\paragraph{Thread Context and Topic Drift}
Including thread history did not improve retrieval performance and, in the case of the 100K dataset, further degraded results. One explanation is that thread context introduces additional noise and redundancy, as conversational history is often shared across multiple emails. This increases similarity between documents and docids, making it harder for DSI to distinguish individual emails. In addition, email threads frequently exhibit topic drift, where the focus shifts over time (e.g., from scheduling to budget discussion), which may not be well captured by a single identifier. Important information may also appear late in the thread or be truncated, further limiting the usefulness of contextual content.

\paragraph{Identifier Design and Learning Objective}
Another contributing factor is the use of a direct maximum likelihood objective that models relevance as a binary signal. In real-world email data, relevance is often graded rather than binary, and near-duplicate or partially relevant emails are common. Treating relevance as binary may confuse the model during training, especially when many emails share similar content or identifiers. This limitation may also explain why increasing the dataset size from 10K to 100K did not lead to improved performance, suggesting overfitting to noise rather than effective corpus memorization.

\paragraph{Pseudo-Query Construction}
The construction of pseudo-queries also introduced noise. Queries were generated by prepending structural markers such as “subject:” and “body:” to email text, which the query generation models were not explicitly designed to handle. As a result, these tokens frequently appeared in generated queries, reducing their semantic quality and further degrading DSI performance. This issue is illustrated in the case study presented in Appendix~\ref{app:compare}.

\paragraph{Implications and Future Directions}
These observations suggest that applying generative retrieval to noisy, real-world email data requires more than architectural changes. More robust data cleaning strategies are necessary, potentially leveraging large



\iffalse
On the current version of the data set, BM25 remains a strong baseline for the Enron corpus, particularly on shorter documents. 
DSI underperformed significantly compared across all metrics. Which stands in stark contrast to the trends in recent literature \cite{10.1145/3580305.3599903, NEURIPS2022_892840a6}, where DSI outperforms sparce retrieval. 
Including thread history did not significalntly bridge the semantic gap and in the case of the 100k dataset, degraded the performance or introduced noise.

In previous works DSI was used on Natural Questions or MS MARCO, which are clean encyclopedic texts. Enron is a real world dataset, which means that it is noisy and unstructured. This makes it harder to semantically map between the pseudo-queries and the docids. 
An other cause for DSI poor performance could be that this research uses the most direct MLE objective, which models relevance as a binary signal. This dichotomy, takes relevance as binary which might not be representative of real world data contained in the Enron dataset. This might confuse the model by "near-duplicates" in the training data.
Further, due to resource limitations the amount of queries generated for training was 2, although earlier works used 10+ \cite{10.1145/3580305.3599903}, which is less then other works. The datasets that included the field identifiers art enacting for generating the doctoqueries, might have learning to mach noise instead of semantics.

Because there are a lot of similar docids, memorizing the mapping within the model weights becomes more difficult. This could also explain why the performance of the context dataset performed worse, because the history is shared among multiple emails, the docids might have become more similar. 
Further explanation why the thread was outperformed, might be because of topic drift. The semantic docids, might not have been able to capture the shift form i.e. "meeting on Tuesday" to "budget approval" later in the tread. Also distinct parts of emails might be at the end of the emails or thread losing unique input. 
Regex cleaning is brittle on natural data. Not all cleaning was successful \ref{app:compare} and artifacting remains. Noise effects neural models disproportionately because they try to learn patterns, whereas BM25 ignorance /noise more easily. 
In the case the context the comparison is not a true comparison, because they don't have consistent field identifiers. 

Because artifacting persisted a more robust way of cleaning is necessary to continue the use of the dataset for this task. To continue the exploration of this dataset with this task, a more robust cleaning strategy is necessary to combat the persisting artifacting . Because the artifacting is caused by real world data, it is inconsistent and it could be interesting to perform the cleaning by Large Language model.

Rehearsal content could be added, especially for the context dataset, to increase performance.
For DSI the thread and no context performance was very close, which could because by truncation. To include the important information of the context \cite{10.1145/3690624.3709435} an extraction of important information could be done.

The inclusion of context has the effect that there are a many duplicates of emails, making a lot of emails similar. A score could be given for emails that are contained in the context and are retrieved. Similarly a score could be given to semantically similar docids or index text, move away form the binary task.

Applying the current method on a curated Q\&A like Natural Questions or the MS MARCO dataset. This could give insight into the efficacy of the used methods. Further, to check if the data has potential, a simpler task could used that works better as a dichotomy. A task like spam detection for instance, which has been done before and was its original use \cite{inproceedings, klimt2004enron}.

To generate the doc to queries the model was instructed by including "subject:" or "body:" followed by the subject and body respectively. This cluttered the queries as the doctoquery model was not made to accept instructions. This had the effect that the words subject and body where included often in the queries \ref{app:schema_impact}, because the model might be sensitive to structural patterns. Thus inducing noise. In further work this could be omitted. 

Currently the Context data set uses the whole context for generating the docids and queries. This might induce clutter and might result in very similar docids and queries.

The context dataset might have had to many topics, to verify a study could be conducted modeling the topics using LDA or calculating the similarity using dense vector embeddings.

An other option for Generative retrieval on the Enron corpus is constructing a model that natively takes in the relationship between emails and users. 

It would also be interesting to see how other baselines perform on the dataset and what the performance increase could be, by optimizing the parameters of BM25.

In appendix \ref{app:compare} shows a case study between BM25 and DSI, here it is visible how the queries generated by the title model spew nonsense  which creates noise for DSI and confuses the model.
\fi

