\documentclass{article}
\usepackage{graphicx}

\usepackage[backend=biber, style=numeric, sorting=nty]{biblatex}
\addbibresource{../Data-study/references.bib}

\newcommand{\titlecite}[1]{%
  \emph{\parencite{#1} \citetitle{#1}}\\%
}
\title{Literary analysis}
\author{DaniÃ«l van Oosteroom}
\date{October 2025}

\begin{document}


\maketitle

\section{Enron Corpus}


\titlecite{noever2020enroncorpusemailbodies}
Although this research is not well peer-reviewed it is full it interesting data mining techniques for document discovery. Techniques like sentiment analysis and decision trees. This is different from the use of GR models but can add to the document discovery.

\bigskip

\titlecite{blanco2020manifestation}
This is a research by a masters student. It talks about linguistic markers of fraud. Although this is not a peer reviewed research it is interesting for inspiration on which type of language might be fraudulent.

\bigskip

\titlecite{klimt2004introducing}
This paper is the original paper that described the Enron corpus. This paper already shows how the data set can be cleaned an analyzed. 

\bigskip

\titlecite{klimt2004enron}
This paper is the bigger whole were the previous paper is part of. And talks about how to use the data for email classification. Although this is a different task then a GR model it is similar enough to take inspiration out of. It talks about which data is most useful for classification en concludes that unstructured data like the subject and the body has the biggest influence on the performance. Numerical data was not very interesting (thinks like word count, character count, number of recipients) and did not contribute to the performance. Lastly there was categorical text fields like to an from which were not very useful for classification. This was not useful for the classification. 


\section{Technical papers}

\titlecite{JMLR:v21:20-074}
The T5 model stands for the Test-to-Text Transfer Transformer. This model is build with the idea that every tasks is just a Test-to-Text task and is fine-tuned on those tasks after the unsupervised learning step. This is also why it is used often with GR as this is also a Test-to-Text task.

\bigskip

\titlecite{NEURIPS2022_892840a6}
This paper is about the first implementation of the Differenciable Search Index (DSI). This explores the possibility to use LLM's as Search Index because this would allow for end to end optimization.

\bigskip

\titlecite{10.1145/3580305.3599903}
This paper explores the method of generating id's for DSI. It proposes a method where the ids for the documents are just semantic descriptions of the documents. These are called Elaborative Descriptions (ED). It also describes how it acquires these descriptions. 


\titlecite{depaulo2003cues}
This is the main papers that discusses deceptive cues. Cues are more high level and markers are more low level. This is a broad overview but not specific to the enron case.

\titlecite{louwerse2010linguistic}
Based on the Louwerse et al. (2010) paper analyzing the Enron corpus, here is the comprehensive list of linguistic cues that were statistically linked to fraudulent events.

The researchers analyzed the data using two distinct models: the **Linguistic Category Model (LCM)** (Study 1) and **Standard Linguistic Cues** (Study 2).

### 1. Linguistic Category Model (LCM) Cues
This was the primary focus of the paper. The researchers found that **Abstractness** was the most reliable predictor of fraud in the social network.

* **Abstractness Score:** An aggregate score derived from the categories below.
    * **Finding:** Emails sent during fraudulent periods had significantly **higher** abstractness scores.
    * **Reasoning:** Abstract language is harder to verify, generates more disagreement, and is less informative, making it ideal for obfuscation.
* **Adjectives (ADJ):** Words referring to qualities or characteristics (e.g., *optimal*, *distraught*).
    * **Finding:** This was the **strongest single predictor**. Events like "Shredding Occurs," "Fraud Announced," and "Bankruptcy Filed" all predicted a higher frequency of adjectives.
* **State Verbs (SV):** Verbs referring to enduring cognitive or emotional states (e.g., *trust*, *understand*).
* **State Action Verbs (SAV):** Verbs referring to the emotional consequence of an action (e.g., *surprise*, *anger*).
* **Interpretative Action Verbs (IAV):** Verbs referring to general classes of behavior without physical invariants (e.g., *help*, *tease*).
* **Descriptive Action Verbs (DAV):** Concrete verbs describing specific actions (e.g., *hit*, *walk*).
    * *Note:* While DAVs are concrete, the study found positive correlations between DAVs and specific crises (like Layoffs), likely due to the need to describe specific logistical actions during those times.

### 2. Standard Linguistic Cues (LIWC-style features)
In Study 2, the researchers tested cues traditionally associated with deception in experimental settings. In the Enron corpus, many of these cues showed relationships **opposite** to what is found in laboratory experiments.

* **First-Person Pronouns:** ($I, me, mine$)
    * **Finding:** **Positive correlation** with fraudulent events (e.g., Fraud Announced, Bankruptcy Filed).
    * *Contrast:* In lab studies, liars usually use *fewer* self-references to dissociate themselves. In the Enron network, executives used *more*.
* **Third-Person Pronouns:** ($he, she, they$)
    * **Finding:** Generally showed a **negative correlation** with events like "Layoffs" and "Fraudulent Paperwork Filed."
* **Analytic Negation:** (e.g., *not*, *no*)
    * **Finding:** **Positive correlation** with events like "SEC Inquiry" and "Shredding Stopped."
* **Causal Adverbs:** (e.g., *because*, *effect*)
    * **Finding:** **Negative correlation** with "Layoffs."
* **Word Count:**
    * **Finding:** Mixed results; "Layoffs" predicted lower word counts (negative), while "Loss Announced" predicted higher word counts (positive).

### Summary Table of Directionality
(Based on Tables 3 & 4 in the text)

| Cue Category | Direction during Enron Fraud Events | Explanation |
| :--- | :--- | :--- |
| **Abstractness (LCM)** | **INCREASE** | Language becomes vague and harder to verify. |
| **Adjectives (ADJ)** | **INCREASE** | High use of descriptive qualities rather than concrete actions. |
| **1st Person Pronouns**| **INCREASE** | Contrary to lab studies; executives may have taken ownership or been more self-focused during crises. |
| **Negations** | **INCREASE** | Higher denial or negative syntax during investigations. |
| **3rd Person Pronouns**| **DECREASE** | Less focus on others during critical paperwork fraud or layoffs. |bigskip


\printbibliography

\end{document}
