@misc{noever2020enroncorpusemailbodies,
  title = {The Enron Corpus: Where the Email Bodies are Buried?},
  author = {David Noever},
  year = {2020},
  eprint = {2001.10374},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR},
  url = {https://arxiv.org/abs/2001.10374},
}
@book{blanco2020manifestation,
  title = {The Manifestation of Fraud in Language: An Enron eMail Corpus Case
           Study on Fraudulent Language Markers},
  author = {Blanco, Sofia},
  year = {2020},
  publisher = {Hofstra University},
}
@inproceedings{klimt2004introducing,
  title = {Introducing the Enron corpus.},
  author = {Klimt, Bryan and Yang, Yiming},
  journal = "Proc. Conf. on Collaboration, Electronic messaging, Anti-Abuse and
             Spam (CEAS), 2004",
  booktitle = {CEAS},
  volume = {45},
  pages = {92--96},
  year = {2004},
}
@misc{enron_dataset,
  author = {Cohen, William W.},
  title = {Enron Email Dataset},
  year = {2015},
  howpublished = {\\url{https://www.cs.cmu.edu/~enron/}},
  note = {Accessed: 2025-10-30},
}
@inproceedings{klimt2004enron,
  author = "Klimt, Bryan and Yang, Yiming",
  editor = "Boulicaut, Jean-Fran{\c{c}}ois and Esposito, Floriana and Giannotti,
            Fosca and Pedreschi, Dino",
  title = "The Enron Corpus: A New Dataset for Email Classification Research",
  booktitle = "Machine Learning: ECML 2004",
  year = "2004",
  publisher = "Springer Berlin Heidelberg",
  address = "Berlin, Heidelberg",
  pages = "217--226",
  abstract = "Automated classification of email messages into user-specific
              folders and information extraction from chronologically ordered
              email streams have become interesting areas in text learning
              research. However, the lack of large benchmark collections has been
              an obstacle for studying the problems and evaluating the solutions.
              In this paper, we introduce the Enron corpus as a new test bed. We
              analyze its suitability with respect to email folder prediction,
              and provide the baseline results of a state-of-the-art classifier
              (Support Vector Machines) under various conditions, including the
              cases of using individual sections (From, To, Subject and body)
              alone as the input to the classifier, and using all the sections in
              combination with regression weights.",
  isbn = "978-3-540-30115-8",
}
@article{JMLR:v21:20-074,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and
            Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter
            J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
           Transformer},
  journal = {Journal of Machine Learning Research},
  year = {2020},
  volume = {21},
  number = {140},
  pages = {1--67},
  url = {http://jmlr.org/papers/v21/20-074.html},
}
@inproceedings{10.1145/3580305.3599903,
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and Chen, Jiangui and
            Zhu, Zuowei and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
  title = {Semantic-Enhanced Differentiable Search Index Inspired by Learning
           Strategies},
  year = {2023},
  isbn = {9798400701030},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3580305.3599903},
  doi = {10.1145/3580305.3599903},
  abstract = {Recently, a new paradigm called Differentiable Search Index (DSI)
              has been proposed for document retrieval, wherein a
              sequence-to-sequence model is learned to directly map queries to
              relevant document identifiers. The key idea behind DSI is to fully
              parameterize traditional ''index-retrieve'' pipelines within a
              single neural model, by encoding all documents in the corpus into
              the model parameters. In essence, DSI needs to resolve two major
              questions: (1) how to assign an identifier to each document, and
              (2) how to learn the associations between a document and its
              identifier. In this work, we propose a Semantic-Enhanced DSI model
              (SE-DSI) motivated by Learning Strategies in the area of Cognitive
              Psychology. Our approach advances original DSI in two ways: (1) For
              the document identifier, we take inspiration from Elaboration
              Strategies in human learning. Specifically, we assign each document
              an Elaborative Description based on the query generation technique,
              which is more meaningful than a string of integers in the original
              DSI; and (2) For the associations between a document and its
              identifier, we take inspiration from Rehearsal Strategies in human
              learning. Specifically, we select fine-grained semantic features
              from a document as Rehearsal Contents to improve document
              memorization. Both the offline and online experiments show improved
              retrieval performance over prevailing baselines.},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
               Discovery and Data Mining},
  pages = {4904–4913},
  numpages = {10},
  keywords = {rehearsal strategies, elaboration strategies, dsi},
  location = {Long Beach, CA, USA},
  series = {KDD '23},
}
@inproceedings{NEURIPS2022_892840a6,
  author = {Tay, Yi and Tran, Vinh and Dehghani, Mostafa and Ni, Jianmo and
            Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe
            and Gupta, Jai and Schuster, Tal and Cohen, William W and Metzler,
            Donald},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho
            and A. Oh},
  pages = {21831--21843},
  publisher = {Curran Associates, Inc.},
  title = {Transformer Memory as a Differentiable Search Index},
  url = {
         \url{https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf}
         },
  volume = {35},
  year = {2022},
}
@misc{FBIEnronCase,
  author = {{Federal Bureau of Investigation}},
  title = {Enron},
  howpublished = {\url{https://www.fbi.gov/history/famous-cases/enron}},
  note = {Accessed: 2025-11-05},
}
@misc{BritannicaEnron,
  author = {Bondarenko, Peter and The Editors of Encyclopaedia Britannica},
  title = {Enron scandal | Summary, Explained, History, \& Facts},
  howpublished = {\url{https://www.britannica.com/event/enron-scandal}},
  year = {2025},
  month = {Sep},
  note = {Accessed: 2025-11-05},
}
@article{INR-019,
  url = {http://dx.doi.org/10.1561/1500000019},
  year = {2009},
  volume = {3},
  journal = {Foundations and Trends® in Information Retrieval},
  title = {The Probabilistic Relevance Framework: BM25 and Beyond},
  doi = {10.1561/1500000019},
  issn = {1554-0669},
  number = {4},
  pages = {333-389},
  author = {Stephen Robertson and Hugo Zaragoza},
}
@article{bengio2003neural,
  author = {Yoshua Bengio and R{\'{e}}jean Ducharme and Pascal Vincent and
            Christian Jauvin},
  title = {A Neural Probabilistic Language Model},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  year = {2003},
  month = {Feb},
}
@inproceedings{NIPS2017_3f5ee243,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
            Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and
            Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R.
            Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Attention is All you Need},
  url = {\url{
         https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
         }},
  volume = {30},
  year = {2017},
}
@inproceedings{SparckJones1975ReportOT,
  title = {Report on the need for and provision of an 'ideal' information
           retrieval test collection},
  author = {Karen Sparck Jones and C. J. van Rijsbergen},
  year = {1975},
  url = {https://api.semanticscholar.org/CorpusID:60988681},
}
@inproceedings{ni-etal-2022-sentence,
  title = "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text
           Models",
  author = "Ni, Jianmo and Hernandez Abrego, Gustavo and Constant, Noah and Ma,
            Ji and Hall, Keith and Cer, Daniel and Yang, Yinfei",
  editor = "Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2022",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.findings-acl.146/",
  doi = "10.18653/v1/2022.findings-acl.146",
  pages = "1864--1874",
  abstract = "We provide the first exploration of sentence embeddings from
              text-to-text transformers (T5) including the effects of scaling up
              sentence encoders to 11B parameters. Sentence embeddings are
              broadly useful for language processing tasks. While T5 achieves
              impressive performance on language tasks, it is unclear how to
              produce sentence embeddings from encoder-decoder models. We
              investigate three methods to construct Sentence-T5 (ST5) models:
              two utilize only the T5 encoder and one using the full T5
              encoder-decoder. We establish a new sentence representation
              transfer benchmark, SentGLUE, which extends the SentEval toolkit to
              nine tasks from the GLUE benchmark. Our encoder-only models
              outperform the previous best models on both SentEval and SentGLUE
              transfer tasks, including semantic textual similarity (STS).
              Scaling up ST5 from millions to billions of parameters shown to
              consistently improve performance. Finally, our encoder-decoder
              method achieves a new state-of-the-art on STS when using sentence
              embeddings.",
}
@article{AHMED2016278,
  title = {A survey of anomaly detection techniques in financial domain},
  journal = {Future Generation Computer Systems},
  volume = {55},
  pages = {278-288},
  year = {2016},
  issn = {0167-739X},
  doi = {https://doi.org/10.1016/j.future.2015.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X15000023},
  author = {Mohiuddin Ahmed and Abdun Naser Mahmood and Md. Rafiqul Islam},
  keywords = {Clustering, Fraud detection, Anomaly detection},
  abstract = {Anomaly detection is an important data analysis task. It is used
              to identify interesting and emerging patterns, trends and anomalies
              from data. Anomaly detection is an important tool to detect
              abnormalities in many different domains including financial fraud
              detection, computer network intrusion, human behavioural analysis,
              gene expression analysis and many more. Recently, in the financial
              sector, there has been renewed interest in research on detection of
              fraudulent activities. There has been a lot of work in the area of
              clustering based unsupervised anomaly detection in the financial
              domain. This paper presents an in-depth survey of various
              clustering based anomaly detection technique and compares them from
              different perspectives. In addition, we discuss the lack of real
              world data and how synthetic data has been used to validate current
              detection techniques.},
}
@misc{casanueva2020efficientintentdetectiondual,
  title = {Efficient Intent Detection with Dual Sentence Encoders},
  author = {Iñigo Casanueva and Tadas Temčinas and Daniela Gerz and Matthew
            Henderson and Ivan Vulić},
  year = {2020},
  eprint = {2003.04807},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  doi = {https://doi.org/10.48550/arXiv.2003.04807},
  url = {https://arxiv.org/abs/2003.04807},
  abstract = {Building conversational systems in new domains and with added
              functionality requires resource-efficient models that work under
              low-data regimes (i.e., in few-shot setups). Motivated by these
              requirements, we introduce intent detection methods backed by
              pretrained dual sentence encoders such as USE and ConveRT. We
              demonstrate the usefulness and wide applicability of the proposed
              intent detectors, showing that: 1) they outperform intent detectors
              based on fine-tuning the full BERT-Large model or using BERT as a
              fixed black-box encoder on three diverse intent detection data
              sets; 2) the gains are especially pronounced in few-shot setups
              (i.e., with only 10 or 30 annotated examples per intent); 3) our
              intent detectors can be trained in a matter of minutes on a single
              CPU; and 4) they are stable across different hyperparameter
              settings. In hope of facilitating and democratizing research
              focused on intention detection, we release our code, as well as a
              new challenging single-domain intent detection dataset comprising
              13,083 annotated examples over 77 intents.},
}
@inproceedings{10.1145/3295750.3298924,
  author = {Qu, Chen and Yang, Liu and Croft, W. Bruce and Zhang, Yongfeng and
            Trippas, Johanne R. and Qiu, Minghui},
  title = {User Intent Prediction in Information-seeking Conversations},
  year = {2019},
  isbn = {9781450360258},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3295750.3298924},
  doi = {10.1145/3295750.3298924},
  abstract = {Conversational assistants are being progressively adopted by the
              general population. However, they are not capable of handling
              complicated information-seeking tasks that involve multiple turns
              of information exchange. Due to the limited communication bandwidth
              in conversational search, it is important for conversational
              assistants to accurately detect and predict user intent in
              information-seeking conversations. In this paper, we investigate
              two aspects of user intent prediction in an information-seeking
              setting. First, we extract features based on the content,
              structural, and sentiment characteristics of a given utterance, and
              use classic machine learning methods to perform user intent
              prediction. We then conduct an in-depth feature importance analysis
              to identify key features in this prediction task. We find that
              structural features contribute most to the prediction performance.
              Given this finding, we construct neural classifiers to incorporate
              context information and achieve better performance without feature
              engineering. Our findings can provide insights into the important
              factors and effective methods of user intent prediction in
              information-seeking conversations.},
  booktitle = {Proceedings of the 2019 Conference on Human Information
               Interaction and Retrieval},
  pages = {25–33},
  numpages = {9},
  keywords = {user intent prediction, multi-turn question answering,
              information-seeking conversations, conversational search},
  location = {Glasgow, Scotland UK},
  series = {CHIIR '19},
}
@article{depaulo2003cues,
  title = {Cues to deception},
  author = {DePaulo, Bella M. and Lindsay, James J. and Malone, Brian E. and
            Muhlenbruck, Laura and Charlton, Kelly and Cooper, Harris},
  journal = {Psychological Bulletin},
  volume = {129},
  number = {1},
  pages = {74},
  year = {2003},
  doi = {10.1037/0033-2909.129.1.74},
  issn = {1939-1455},
  publisher = {American Psychological Association},
  abstract = {Do people behave differently when they are lying compared with
              when they are telling the truth? The combined results of 1,338
              estimates of 158 cues to deception are reported. Results show that
              in some ways, liars are less forthcoming than truth tellers, and
              they tell less compelling tales. They also make a more negative
              impression and are more tense. Their stories include fewer ordinary
              imperfections and unusual contents. However, many behaviors showed
              no discernible links, or only weak links, to deceit. Cues to
              deception were more pronounced when people were motivated to
              succeed, especially when the motivations were identity relevant
              rather than monetary or material. Cues to deception were also
              stronger when lies were about transgressions. (PsycInfo Database
              Record (c) 2025 APA, all rights reserved)},
}
@inproceedings{louwerse2010linguistic,
  title = {Linguistic cues predict fraudulent events in a corporate social
           network},
  author = {Louwerse, Max M. and Lin, King-Ip and Drescher, Amanda and Semin, G{
            \"u}n R.},
  booktitle = {Proceedings of the 32nd Annual Conference of the Cognitive
               Science Society},
  pages = {961--966},
  year = {2010},
  editor = {Ohlsson, S. and Catrambone, R.},
  publisher = {Cognitive Science Society},
  address = {Austin, TX},
}
@misc{zhuang2022dsiqg,
  author = {Zhuang, Shengyao},
  title = {DSI-QG: Bridging the Gap Between Indexing and Retrieval for
           Differentiable Search Index with Query Generation},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/ArvinZhuang/DSI-QG}},
}
@article{LI2023103475,
  title = {Generative retrieval for conversational question answering},
  journal = {Information Processing \& Management},
  volume = {60},
  number = {5},
  pages = {103475},
  year = {2023},
  issn = {0306-4573},
  doi = {https://doi.org/10.1016/j.ipm.2023.103475},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457323002121},
  author = {Yongqi Li and Nan Yang and Liang Wang and Furu Wei and Wenjie Li},
  keywords = {Conversational question answering, Generative retrieval},
}
@inproceedings{10.1145/3690624.3709435,
author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten and Liu, Shihao and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
title = {Generative Retrieval for Book Search},
year = {2025},
isbn = {9798400712456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3690624.3709435},
doi = {10.1145/3690624.3709435},
abstract = {In book search, relevant book information should be returned in response to a query. Books contain complex, multi-faceted information such as metadata, outlines, and main text, where the outline provides hierarchical information between chapters and sections. Generative retrieval (GR) is a new retrieval paradigm that consolidates corpus information into a single model to generate identifiers of documents that are relevant to a given query. How can GR be applied to book search? Directly applying GR to book search is a challenge due to the unique characteristics of book search: The model needs to retain the complex, multi-faceted information of the book, which increases the demand for labeled data. Splitting book information and treating it as a collection of separate segments for learning might result in a loss of hierarchical information.We propose an effective Generative retrieval framework for Book Search (GBS) that features two main components: data augmentation and outline-oriented book encoding. For data augmentation, GBS constructs multiple query-book pairs for training; it constructs multiple book identifiers based on the outline, various forms of book contents, and simulates real book retrieval scenarios with varied pseudo-queries. This includes coverage-promoting book identifier augmentation, allowing the model to learn to index effectively, and diversity-enhanced query augmentation, allowing the model to learn to retrieve effectively. Outline-oriented book encoding improves length extrapolation through bi-level positional encoding and retentive attention mechanisms to maintain context over long sequences. Experiments on a proprietary Baidu dataset demonstrate that GBS outperforms strong baselines, achieving a 9.8\% improvement in terms of MRR@20, over the state-of-the-art RIPOR method. Experiments on public datasets confirm the robustness and generalizability of GBS, highlighting its potential to enhance book retrieval.},
booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.1},
pages = {2606–2617},
numpages = {12},
keywords = {book retrieval, generative models, generative retrieval},
location = {Toronto ON, Canada},
series = {KDD '25}
}
@techreport{ilprints422,
          number = {1999-66},
           month = {November},
          author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
            note = {Previous number = SIDL-WP-1999-0120},
           title = {The PageRank Citation Ranking: Bringing Order to the Web.},
            type = {Technical Report},
       publisher = {Stanford InfoLab},
            year = {1999},
     institution = {Stanford InfoLab},
             url = {http://ilpubs.stanford.edu:8090/422/},
        abstract = {The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a mathod for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation.}
}
@misc{pleban2020t5headline,
  author       = {Michal Pleban},
  title        = {T5-base-en-generate-headline},
  year         = {2020},
  publisher    = {Hugging Face},
  howpublished = {\url{https://huggingface.co/Michau/t5-base-en-generate-headline}},
  note         = {Hugging Face Model Hub}
}
@inproceedings{karpukhin-etal-2020-dense,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550/",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks."
}
