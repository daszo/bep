@misc{noever2020enroncorpusemailbodies,
  title = {The Enron Corpus: Where the Email Bodies are Buried?},
  author = {David Noever},
  year = {2020},
  eprint = {2001.10374},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR},
  url = {https://arxiv.org/abs/2001.10374},
}
@inproceedings{tang2023recent,
  title = {Recent Advances in Generative Information Retrieval},
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten
            },
  booktitle = {Proceedings of the Annual International ACM SIGIR Conference on
               Research and Development in Information Retrieval in the Asia
               Pacific Region},
  pages = {294--297},
  year = {2023},
}

@inproceedings{chen2022corpusbrain,
  title = {CorpusBrain: Pre-train a Generative Retrieval Model for
           Knowledge-Intensive Language Tasks},
  author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and Liu, Yiqun and
            Fan, Yixing and Cheng, Xueqi},
  booktitle = {Proceedings of the 31st ACM International Conference on
               Information \& Knowledge Management},
  year = {2022},
  pages = {191-200},
}
@inproceedings{chen-2023-unified,
  author = {Chen, Jiangui and Zhang, Ruqing and Guo, Jiafeng and de Rijke,
            Maarten and Liu, Yiqun and Fan, Yixing and Cheng, Xueqi},
  booktitle = {Proceedings of the 2023 ACM SIGIR International Conference on
               Theory of Information Retrieval},
  title = {A Unified Generative Retriever for Knowledge-Intensive Language Tasks
           via Prompt Learning},
  pages = {1448--1457},
  year = {2023},
}
@book{blanco2020manifestation,
  title = {The Manifestation of Fraud in Language: An Enron eMail Corpus Case
           Study on Fraudulent Language Markers},
  author = {Blanco, Sofia},
  year = {2020},
  publisher = {Hofstra University},
}
@inproceedings{genre,
  title = {Autoregressive Entity Retrieval},
  author = {De Cao, Nicola and Izacard, Gautier and Riedel, Sebastian and
            Petroni, Fabio},
  booktitle = {International Conference on Learning Representations},
  year = {2021},
}
@inproceedings{klimt2004introducing,
  title = {Introducing the Enron corpus.},
  author = {Klimt, Bryan and Yang, Yiming},
  journal = "Proc. Conf. on Collaboration, Electronic messaging, Anti-Abuse and
             Spam (CEAS), 2004",
  booktitle = {CEAS},
  volume = {45},
  pages = {92--96},
  year = {2004},
}
@misc{enron_dataset,
  author = {Cohen, William W.},
  title = {Enron Email Dataset},
  year = {2015},
  howpublished = {https://www.cs.cmu.edu/~enron/},
  note = {Accessed: 2025-10-30},
}
@inproceedings{klimt2004enron,
  author = "Klimt, Bryan and Yang, Yiming",
  editor = "Boulicaut, Jean-Fran{\c{c}}ois and Esposito, Floriana and Giannotti,
            Fosca and Pedreschi, Dino",
  title = "The Enron Corpus: A New Dataset for Email Classification Research",
  booktitle = "Machine Learning: ECML 2004",
  year = "2004",
  publisher = "Springer Berlin Heidelberg",
  address = "Berlin, Heidelberg",
  pages = "217--226",
  abstract = "Automated classification of email messages into user-specific
              folders and information extraction from chronologically ordered
              email streams have become interesting areas in text learning
              research. However, the lack of large benchmark collections has been
              an obstacle for studying the problems and evaluating the solutions.
              In this paper, we introduce the Enron corpus as a new test bed. We
              analyze its suitability with respect to email folder prediction,
              and provide the baseline results of a state-of-the-art classifier
              (Support Vector Machines) under various conditions, including the
              cases of using individual sections (From, To, Subject and body)
              alone as the input to the classifier, and using all the sections in
              combination with regression weights.",
  isbn = "978-3-540-30115-8",
}
@article{JMLR:v21:20-074,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and
            Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter
            J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
           Transformer},
  journal = {Journal of Machine Learning Research},
  year = {2020},
  volume = {21},
  number = {140},
  pages = {1--67},
  url = {http://jmlr.org/papers/v21/20-074.html},
}
@inproceedings{10.1145/3580305.3599903,
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and Chen, Jiangui and
            Zhu, Zuowei and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
  title = {Semantic-Enhanced Differentiable Search Index Inspired by Learning
           Strategies},
  year = {2023},
  isbn = {9798400701030},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3580305.3599903},
  doi = {10.1145/3580305.3599903},
  abstract = {Recently, a new paradigm called Differentiable Search Index (DSI)
              has been proposed for document retrieval, wherein a
              sequence-to-sequence model is learned to directly map queries to
              relevant document identifiers. The key idea behind DSI is to fully
              parameterize traditional ''index-retrieve'' pipelines within a
              single neural model, by encoding all documents in the corpus into
              the model parameters. In essence, DSI needs to resolve two major
              questions: (1) how to assign an identifier to each document, and
              (2) how to learn the associations between a document and its
              identifier. In this work, we propose a Semantic-Enhanced DSI model
              (SE-DSI) motivated by Learning Strategies in the area of Cognitive
              Psychology. Our approach advances original DSI in two ways: (1) For
              the document identifier, we take inspiration from Elaboration
              Strategies in human learning. Specifically, we assign each document
              an Elaborative Description based on the query generation technique,
              which is more meaningful than a string of integers in the original
              DSI; and (2) For the associations between a document and its
              identifier, we take inspiration from Rehearsal Strategies in human
              learning. Specifically, we select fine-grained semantic features
              from a document as Rehearsal Contents to improve document
              memorization. Both the offline and online experiments show improved
              retrieval performance over prevailing baselines.},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
               Discovery and Data Mining},
  pages = {4904–4913},
  numpages = {10},
  keywords = {rehearsal strategies, elaboration strategies, dsi},
  location = {Long Beach, CA, USA},
  series = {KDD '23},
}
@inproceedings{NEURIPS2022_892840a6,
  author = {Tay, Yi and Tran, Vinh and Dehghani, Mostafa and Ni, Jianmo and
            Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe
            and Gupta, Jai and Schuster, Tal and Cohen, William W and Metzler,
            Donald},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho
            and A. Oh},
  pages = {21831--21843},
  publisher = {Curran Associates, Inc.},
  title = {Transformer Memory as a Differentiable Search Index},
  url = {
         https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf
         },
  volume = {35},
  year = {2022},
}
@misc{FBIEnronCase,
  author = {{Federal Bureau of Investigation}},
  title = {Enron},
  howpublished = {https://www.fbi.gov/history/famous-cases/enron},
  note = {Accessed: 2025-11-05},
}
@misc{BritannicaEnron,
  author = {Bondarenko, Peter and The Editors of Encyclopaedia Britannica},
  title = {Enron scandal | Summary, Explained, History, \& Facts},
  howpublished = {https://www.britannica.com/event/enron-scandal},
  year = {2025},
  month = {Sep},
  note = {Accessed: 2025-11-05},
}
@article{INR-019,
  url = {http://dx.doi.org/10.1561/1500000019},
  year = {2009},
  volume = {3},
  journal = {Foundations and Trends® in Information Retrieval},
  title = {The Probabilistic Relevance Framework: BM25 and Beyond},
  doi = {10.1561/1500000019},
  issn = {1554-0669},
  number = {4},
  pages = {333-389},
  author = {Stephen Robertson and Hugo Zaragoza},
}
@article{bengio2003neural,
  author = {Yoshua Bengio and R{\'{e}}jean Ducharme and Pascal Vincent and
            Christian Jauvin},
  title = {A Neural Probabilistic Language Model},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  year = {2003},
  month = {Feb},
}
@inproceedings{NIPS2017_3f5ee243,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
            Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and
            Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R.
            Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Attention is All you Need},
  url = {
         https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
         },
  volume = {30},
  year = {2017},
}
@inproceedings{SparckJones1975ReportOT,
  title = {Report on the need for and provision of an 'ideal' information
           retrieval test collection},
  author = {Karen Sparck Jones and C. J. van Rijsbergen},
  year = {1975},
  url = {https://api.semanticscholar.org/CorpusID:60988681},
}
@inproceedings{ni-etal-2022-sentence,
  title = "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text
           Models",
  author = "Ni, Jianmo and Hernandez Abrego, Gustavo and Constant, Noah and Ma,
            Ji and Hall, Keith and Cer, Daniel and Yang, Yinfei",
  editor = "Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2022",
  month = may,
  year = "2022",
  address = "Dublin, Ireland",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2022.findings-acl.146/",
  doi = "10.18653/v1/2022.findings-acl.146",
  pages = "1864--1874",
  abstract = "We provide the first exploration of sentence embeddings from
              text-to-text transformers (T5) including the effects of scaling up
              sentence encoders to 11B parameters. Sentence embeddings are
              broadly useful for language processing tasks. While T5 achieves
              impressive performance on language tasks, it is unclear how to
              produce sentence embeddings from encoder-decoder models. We
              investigate three methods to construct Sentence-T5 (ST5) models:
              two utilize only the T5 encoder and one using the full T5
              encoder-decoder. We establish a new sentence representation
              transfer benchmark, SentGLUE, which extends the SentEval toolkit to
              nine tasks from the GLUE benchmark. Our encoder-only models
              outperform the previous best models on both SentEval and SentGLUE
              transfer tasks, including semantic textual similarity (STS).
              Scaling up ST5 from millions to billions of parameters shown to
              consistently improve performance. Finally, our encoder-decoder
              method achieves a new state-of-the-art on STS when using sentence
              embeddings.",
}
@article{AHMED2016278,
  title = {A survey of anomaly detection techniques in financial domain},
  journal = {Future Generation Computer Systems},
  volume = {55},
  pages = {278-288},
  year = {2016},
  issn = {0167-739X},
  doi = {https://doi.org/10.1016/j.future.2015.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0167739X15000023},
  author = {Mohiuddin Ahmed and Abdun Naser Mahmood and Md. Rafiqul Islam},
  keywords = {Clustering, Fraud detection, Anomaly detection},
  abstract = {Anomaly detection is an important data analysis task. It is used
              to identify interesting and emerging patterns, trends and anomalies
              from data. Anomaly detection is an important tool to detect
              abnormalities in many different domains including financial fraud
              detection, computer network intrusion, human behavioural analysis,
              gene expression analysis and many more. Recently, in the financial
              sector, there has been renewed interest in research on detection of
              fraudulent activities. There has been a lot of work in the area of
              clustering based unsupervised anomaly detection in the financial
              domain. This paper presents an in-depth survey of various
              clustering based anomaly detection technique and compares them from
              different perspectives. In addition, we discuss the lack of real
              world data and how synthetic data has been used to validate current
              detection techniques.},
}
@misc{casanueva2020efficientintentdetectiondual,
  title = {Efficient Intent Detection with Dual Sentence Encoders},
  author = {Iñigo Casanueva and Tadas Temčinas and Daniela Gerz and Matthew
            Henderson and Ivan Vulić},
  year = {2020},
  eprint = {2003.04807},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  doi = {https://doi.org/10.48550/arXiv.2003.04807},
  url = {https://arxiv.org/abs/2003.04807},
  abstract = {Building conversational systems in new domains and with added
              functionality requires resource-efficient models that work under
              low-data regimes (i.e., in few-shot setups). Motivated by these
              requirements, we introduce intent detection methods backed by
              pretrained dual sentence encoders such as USE and ConveRT. We
              demonstrate the usefulness and wide applicability of the proposed
              intent detectors, showing that: 1) they outperform intent detectors
              based on fine-tuning the full BERT-Large model or using BERT as a
              fixed black-box encoder on three diverse intent detection data
              sets; 2) the gains are especially pronounced in few-shot setups
              (i.e., with only 10 or 30 annotated examples per intent); 3) our
              intent detectors can be trained in a matter of minutes on a single
              CPU; and 4) they are stable across different hyperparameter
              settings. In hope of facilitating and democratizing research
              focused on intention detection, we release our code, as well as a
              new challenging single-domain intent detection dataset comprising
              13,083 annotated examples over 77 intents.},
}
@inproceedings{10.1145/3295750.3298924,
  author = {Qu, Chen and Yang, Liu and Croft, W. Bruce and Zhang, Yongfeng and
            Trippas, Johanne R. and Qiu, Minghui},
  title = {User Intent Prediction in Information-seeking Conversations},
  year = {2019},
  isbn = {9781450360258},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3295750.3298924},
  doi = {10.1145/3295750.3298924},
  abstract = {Conversational assistants are being progressively adopted by the
              general population. However, they are not capable of handling
              complicated information-seeking tasks that involve multiple turns
              of information exchange. Due to the limited communication bandwidth
              in conversational search, it is important for conversational
              assistants to accurately detect and predict user intent in
              information-seeking conversations. In this paper, we investigate
              two aspects of user intent prediction in an information-seeking
              setting. First, we extract features based on the content,
              structural, and sentiment characteristics of a given utterance, and
              use classic machine learning methods to perform user intent
              prediction. We then conduct an in-depth feature importance analysis
              to identify key features in this prediction task. We find that
              structural features contribute most to the prediction performance.
              Given this finding, we construct neural classifiers to incorporate
              context information and achieve better performance without feature
              engineering. Our findings can provide insights into the important
              factors and effective methods of user intent prediction in
              information-seeking conversations.},
  booktitle = {Proceedings of the 2019 Conference on Human Information
               Interaction and Retrieval},
  pages = {25–33},
  numpages = {9},
  keywords = {user intent prediction, multi-turn question answering,
              information-seeking conversations, conversational search},
  location = {Glasgow, Scotland UK},
  series = {CHIIR '19},
}
@article{depaulo2003cues,
  title = {Cues to deception},
  author = {DePaulo, Bella M. and Lindsay, James J. and Malone, Brian E. and
            Muhlenbruck, Laura and Charlton, Kelly and Cooper, Harris},
  journal = {Psychological Bulletin},
  volume = {129},
  number = {1},
  pages = {74},
  year = {2003},
  doi = {10.1037/0033-2909.129.1.74},
  issn = {1939-1455},
  publisher = {American Psychological Association},
  abstract = {Do people behave differently when they are lying compared with
              when they are telling the truth? The combined results of 1,338
              estimates of 158 cues to deception are reported. Results show that
              in some ways, liars are less forthcoming than truth tellers, and
              they tell less compelling tales. They also make a more negative
              impression and are more tense. Their stories include fewer ordinary
              imperfections and unusual contents. However, many behaviors showed
              no discernible links, or only weak links, to deceit. Cues to
              deception were more pronounced when people were motivated to
              succeed, especially when the motivations were identity relevant
              rather than monetary or material. Cues to deception were also
              stronger when lies were about transgressions. (PsycInfo Database
              Record (c) 2025 APA, all rights reserved)},
}
@inproceedings{louwerse2010linguistic,
  title = {Linguistic cues predict fraudulent events in a corporate social
           network},
  author = {Louwerse, Max M. and Lin, King-Ip and Drescher, Amanda and Semin, G{
            \"u}n R.},
  booktitle = {Proceedings of the 32nd Annual Conference of the Cognitive
               Science Society},
  pages = {961--966},
  year = {2010},
  editor = {Ohlsson, S. and Catrambone, R.},
  publisher = {Cognitive Science Society},
  address = {Austin, TX},
}
@misc{zhuang2022dsiqg,
  author = {Zhuang, Shengyao},
  title = {DSI-QG: Bridging the Gap Between Indexing and Retrieval for
           Differentiable Search Index with Query Generation},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {https://github.com/ArvinZhuang/DSI-QG},
}
@article{LI2023103475,
  title = {Generative retrieval for conversational question answering},
  journal = {Information Processing \& Management},
  volume = {60},
  number = {5},
  pages = {103475},
  year = {2023},
  issn = {0306-4573},
  doi = {https://doi.org/10.1016/j.ipm.2023.103475},
  url = {https://www.sciencedirect.com/science/article/pii/S0306457323002121},
  author = {Yongqi Li and Nan Yang and Liang Wang and Furu Wei and Wenjie Li},
  keywords = {Conversational question answering, Generative retrieval},
}
@inproceedings{10.1145/3690624.3709435,
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and de Rijke, Maarten
            and Liu, Shihao and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
  title = {Generative Retrieval for Book Search},
  year = {2025},
  isbn = {9798400712456},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3690624.3709435},
  doi = {10.1145/3690624.3709435},
  abstract = {In book search, relevant book information should be returned in
              response to a query. Books contain complex, multi-faceted
              information such as metadata, outlines, and main text, where the
              outline provides hierarchical information between chapters and
              sections. Generative retrieval (GR) is a new retrieval paradigm
              that consolidates corpus information into a single model to
              generate identifiers of documents that are relevant to a given
              query. How can GR be applied to book search? Directly applying GR
              to book search is a challenge due to the unique characteristics of
              book search: The model needs to retain the complex, multi-faceted
              information of the book, which increases the demand for labeled
              data. Splitting book information and treating it as a collection of
              separate segments for learning might result in a loss of
              hierarchical information.We propose an effective Generative
              retrieval framework for Book Search (GBS) that features two main
              components: data augmentation and outline-oriented book encoding.
              For data augmentation, GBS constructs multiple query-book pairs for
              training; it constructs multiple book identifiers based on the
              outline, various forms of book contents, and simulates real book
              retrieval scenarios with varied pseudo-queries. This includes
              coverage-promoting book identifier augmentation, allowing the model
              to learn to index effectively, and diversity-enhanced query
              augmentation, allowing the model to learn to retrieve effectively.
              Outline-oriented book encoding improves length extrapolation
              through bi-level positional encoding and retentive attention
              mechanisms to maintain context over long sequences. Experiments on
              a proprietary Baidu dataset demonstrate that GBS outperforms strong
              baselines, achieving a 9.8\% improvement in terms of MRR@20, over
              the state-of-the-art RIPOR method. Experiments on public datasets
              confirm the robustness and generalizability of GBS, highlighting
              its potential to enhance book retrieval.},
  booktitle = {Proceedings of the 31st ACM SIGKDD Conference on Knowledge
               Discovery and Data Mining V.1},
  pages = {2606–2617},
  numpages = {12},
  keywords = {book retrieval, generative models, generative retrieval},
  location = {Toronto ON, Canada},
  series = {KDD '25},
}
@techreport{ilprints422,
  number = {1999-66},
  month = {November},
  author = {Lawrence Page and Sergey Brin and Rajeev Motwani and Terry Winograd},
  note = {Previous number = SIDL-WP-1999-0120},
  title = {The PageRank Citation Ranking: Bringing Order to the Web.},
  type = {Technical Report},
  publisher = {Stanford InfoLab},
  year = {1999},
  institution = {Stanford InfoLab},
  url = {http://ilpubs.stanford.edu:8090/422/},
  abstract = {The importance of a Web page is an inherently subjective matter,
              which depends on the readers interests, knowledge and attitudes.
              But there is still much that can be said objectively about the
              relative importance of Web pages. This paper describes PageRank, a
              mathod for rating Web pages objectively and mechanically,
              effectively measuring the human interest and attention devoted to
              them. We compare PageRank to an idealized random Web surfer. We
              show how to efficiently compute PageRank for large numbers of
              pages. And, we show how to apply PageRank to search and to user
              navigation.},
}
@misc{pleban2020t5headline,
  author = {Michal Pleban},
  title = {T5-base-en-generate-headline},
  year = {2020},
  publisher = {Hugging Face},
  howpublished = {https://huggingface.co/Michau/t5-base-en-generate-headline},
  note = {Hugging Face Model Hub},
}
@inproceedings{karpukhin-etal-2020-dense,
  title = "Dense Passage Retrieval for Open-Domain Question Answering",
  author = "Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis,
            Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih,
            Wen-tau",
  editor = "Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in
               Natural Language Processing (EMNLP)",
  month = nov,
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = {https://aclanthology.org/2020.emnlp-main.550/},
  doi = "10.18653/v1/2020.emnlp-main.550",
  pages = "6769--6781",
  abstract = "Open-domain question answering relies on efficient passage
              retrieval to select candidate contexts, where traditional sparse
              vector space models, such as TF-IDF or BM25, are the de facto
              method. In this work, we show that retrieval can be practically
              implemented using dense representations alone, where embeddings are
              learned from a small number of questions and passages by a simple
              dual-encoder framework. When evaluated on a wide range of
              open-domain QA datasets, our dense retriever outperforms a strong
              Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of
              top-20 passage retrieval accuracy, and helps our end-to-end QA
              system establish new state-of-the-art on multiple open-domain QA
              benchmarks.",
}
@inproceedings{inproceedings,
  author = {Mark, Gloria and Iqbal, Shamsi and Czerwinski, Mary and Johns, Paul
            and Sano, Akane and Lutchyn, Yuliya},
  year = {2016},
  month = {05},
  pages = {1717-1728},
  title = {Email Duration, Batching and Self-interruption: Patterns of Email Use
           on Productivity and Stress},
  doi = {10.1145/2858036.2858262},
}
@misc{Adobe2019,
  author = {{Adobe}},
  title = {If {You} {Think} {Email} {Is} {Dead}, {Think} {Again}: {The} 2019 {
           Adobe} {Email} {Usage} {Study}},
  year = {2019},
  month = {September},
  howpublished = {\url{
                  https://blog.adobe.com/en/publish/2019/09/08/if-you-think-email-is-dead-think-again
                  }},
  note = {Accessed: 2025-05-23},
}

@book{strathprints2621,
  year = {2004},
  title = {Introduction to modern information retrieval},
  publisher = {Facet Publishing},
  author = {Chowdhury, G.},
  abstract = {An information retrieval system is designed to analyse, process
              and store sources of information and retrieve those that match a
              particular user's requirements. A bewildering range of techniques
              is now available to the information professional attempting to
              achieve this goal. It is recognized that today's information
              professionals need to concentrate their efforts on learning the
              techniques of computerized information retrieval. However, it is
              this book's contention that it also benefits them to learn the
              theory, techniques and tools that constitute the traditional
              approaches to the organization and processing of information. In
              fact much of this knowledge may still be applicable in the storage
              and retrieval of electronic information in digital library
              environments.},
  url = {https://strathprints.strath.ac.uk/2621/},
  isbn = {1-85604-480-7},
}
@article{2022arXiv220610128Z,
  author = {{Zhuang}, Shengyao and {Ren}, Houxing and {Shou}, Linjun and {Pei},
            Jian and {Gong}, Ming and {Zuccon}, Guido and {Jiang}, Daxin},
  title = "{Bridging the Gap Between Indexing and Retrieval for Differentiable
           Search Index with Query Generation}",
  journal = {arXiv e-prints},
  keywords = {Computer Science - Information Retrieval, Computer Science -
              Computation and Language},
  year = 2022,
  month = jun,
  eid = {arXiv:2206.10128},
  pages = {arXiv:2206.10128},
  doi = {10.48550/arXiv.2206.10128},
  archivePrefix = {arXiv},
  eprint = {2206.10128},
  primaryClass = {cs.IR},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2022arXiv220610128Z},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}

@inproceedings{DBLP:conf/sigir/LinMLYPN21,
  author = {Jimmy Lin and Xueguang Ma and Sheng-Chieh Lin and Jheng-Hong Yang
            and Ronak Pradeep and Rodrigo Frassetto Nogueira},
  title = {Pyserini: A Python Toolkit for Reproducible Information Retrieval
           Research with Sparse and Dense Representations},
  year = {2021},
  cdate = {1609459200000},
  pages = {2356-2362},
  url = {https://doi.org/10.1145/3404835.3463238},
  booktitle = {SIGIR},
  crossref = {conf/sigir/2021},
}
@misc{barrios2016variationssimilarityfunctiontextrank,
  title = {Variations of the Similarity Function of TextRank for Automated
           Summarization},
  author = {Federico Barrios and Federico López and Luis Argerich and Rosa
            Wachenchauzer},
  year = {2016},
  eprint = {1602.03606},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/1602.03606},
}

inproceedings{inproceedings,
author = {Metsis, Vangelis and Androutsopoulos, Ion and Paliouras, Georgios},
year = {2006},
month = {01},
pages = {},
title = {Spam Filtering with Naive Bayes - Which Naive Bayes?},
journal = {In CEAS}
}
@article{2021arXiv210311943K,
  author = {{Koroteev}, M.~V.},
  title = "{BERT: A Review of Applications in Natural Language Processing and
           Understanding}",
  journal = {arXiv e-prints},
  keywords = {Computer Science - Computation and Language, Computer Science -
              Artificial Intelligence, Computer Science - Machine Learning},
  year = 2021,
  month = mar,
  eid = {arXiv:2103.11943},
  pages = {arXiv:2103.11943},
  doi = {10.48550/arXiv.2103.11943},
  archivePrefix = {arXiv},
  eprint = {2103.11943},
  primaryClass = {cs.CL},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2021arXiv210311943K},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
}
