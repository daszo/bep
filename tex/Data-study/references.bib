@misc{noever2020enroncorpusemailbodies,
  title = {The Enron Corpus: Where the Email Bodies are Buried?},
  author = {David Noever},
  year = {2020},
  eprint = {2001.10374},
  archivePrefix = {arXiv},
  primaryClass = {cs.IR},
  url = {https://arxiv.org/abs/2001.10374},
}
@book{blanco2020manifestation,
  title = {The Manifestation of Fraud in Language: An Enron eMail Corpus Case
           Study on Fraudulent Language Markers},
  author = {Blanco, Sofia},
  year = {2020},
  publisher = {Hofstra University},
}
@inproceedings{klimt2004introducing,
  title = {Introducing the Enron corpus.},
  author = {Klimt, Bryan and Yang, Yiming},
  journal = "Proc. Conf. on Collaboration, Electronic messaging, Anti-Abuse and
             Spam (CEAS), 2004",
  booktitle = {CEAS},
  volume = {45},
  pages = {92--96},
  year = {2004},
}
@misc{enron_dataset,
  author = {Cohen, William W.},
  title = {Enron Email Dataset},
  year = {2015},
  howpublished = {\\url{https://www.cs.cmu.edu/~enron/}},
  note = {Accessed: 2025-10-30},
}
@inproceedings{klimt2004enron,
  author = "Klimt, Bryan and Yang, Yiming",
  editor = "Boulicaut, Jean-Fran{\c{c}}ois and Esposito, Floriana and Giannotti,
            Fosca and Pedreschi, Dino",
  title = "The Enron Corpus: A New Dataset for Email Classification Research",
  booktitle = "Machine Learning: ECML 2004",
  year = "2004",
  publisher = "Springer Berlin Heidelberg",
  address = "Berlin, Heidelberg",
  pages = "217--226",
  abstract = "Automated classification of email messages into user-specific
              folders and information extraction from chronologically ordered
              email streams have become interesting areas in text learning
              research. However, the lack of large benchmark collections has been
              an obstacle for studying the problems and evaluating the solutions.
              In this paper, we introduce the Enron corpus as a new test bed. We
              analyze its suitability with respect to email folder prediction,
              and provide the baseline results of a state-of-the-art classifier
              (Support Vector Machines) under various conditions, including the
              cases of using individual sections (From, To, Subject and body)
              alone as the input to the classifier, and using all the sections in
              combination with regression weights.",
  isbn = "978-3-540-30115-8",
}
@article{JMLR:v21:20-074,
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and
            Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter
            J. Liu},
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
           Transformer},
  journal = {Journal of Machine Learning Research},
  year = {2020},
  volume = {21},
  number = {140},
  pages = {1--67},
  url = {http://jmlr.org/papers/v21/20-074.html},
}
@inproceedings{10.1145/3580305.3599903,
  author = {Tang, Yubao and Zhang, Ruqing and Guo, Jiafeng and Chen, Jiangui and
            Zhu, Zuowei and Wang, Shuaiqiang and Yin, Dawei and Cheng, Xueqi},
  title = {Semantic-Enhanced Differentiable Search Index Inspired by Learning
           Strategies},
  year = {2023},
  isbn = {9798400701030},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3580305.3599903},
  doi = {10.1145/3580305.3599903},
  abstract = {Recently, a new paradigm called Differentiable Search Index (DSI)
              has been proposed for document retrieval, wherein a
              sequence-to-sequence model is learned to directly map queries to
              relevant document identifiers. The key idea behind DSI is to fully
              parameterize traditional ''index-retrieve'' pipelines within a
              single neural model, by encoding all documents in the corpus into
              the model parameters. In essence, DSI needs to resolve two major
              questions: (1) how to assign an identifier to each document, and
              (2) how to learn the associations between a document and its
              identifier. In this work, we propose a Semantic-Enhanced DSI model
              (SE-DSI) motivated by Learning Strategies in the area of Cognitive
              Psychology. Our approach advances original DSI in two ways: (1) For
              the document identifier, we take inspiration from Elaboration
              Strategies in human learning. Specifically, we assign each document
              an Elaborative Description based on the query generation technique,
              which is more meaningful than a string of integers in the original
              DSI; and (2) For the associations between a document and its
              identifier, we take inspiration from Rehearsal Strategies in human
              learning. Specifically, we select fine-grained semantic features
              from a document as Rehearsal Contents to improve document
              memorization. Both the offline and online experiments show improved
              retrieval performance over prevailing baselines.},
  booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge
               Discovery and Data Mining},
  pages = {4904–4913},
  numpages = {10},
  keywords = {rehearsal strategies, elaboration strategies, dsi},
  location = {Long Beach, CA, USA},
  series = {KDD '23},
}
@inproceedings{NEURIPS2022_892840a6,
  author = {Tay, Yi and Tran, Vinh and Dehghani, Mostafa and Ni, Jianmo and
            Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe
            and Gupta, Jai and Schuster, Tal and Cohen, William W and Metzler,
            Donald},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho
            and A. Oh},
  pages = {21831--21843},
  publisher = {Curran Associates, Inc.},
  title = {Transformer Memory as a Differentiable Search Index},
  url = {
         https://proceedings.neurips.cc/paper_files/paper/2022/file/892840a6123b5ec99ebaab8be1530fba-Paper-Conference.pdf
         },
  volume = {35},
  year = {2022},
}
@misc{FBIEnronCase,
  author = {{Federal Bureau of Investigation}},
  title = {Enron},
  howpublished = {\url{https://www.fbi.gov/history/famous-cases/enron}},
  note = {Accessed: 2025-11-05},
}
@misc{BritannicaEnron,
  author = {Bondarenko, Peter and The Editors of Encyclopaedia Britannica},
  title = {Enron scandal | Summary, Explained, History, \& Facts},
  howpublished = {\url{https://www.britannica.com/event/enron-scandal}},
  year = {2025},
  month = {Sep},
  note = {Accessed: 2025-11-05},
}
@article{INR-019,
  url = {http://dx.doi.org/10.1561/1500000019},
  year = {2009},
  volume = {3},
  journal = {Foundations and Trends® in Information Retrieval},
  title = {The Probabilistic Relevance Framework: BM25 and Beyond},
  doi = {10.1561/1500000019},
  issn = {1554-0669},
  number = {4},
  pages = {333-389},
  author = {Stephen Robertson and Hugo Zaragoza},
}
@article{bengio2003neural,
  author = {Yoshua Bengio and R{\'{e}}jean Ducharme and Pascal Vincent and
            Christian Jauvin},
  title = {A Neural Probabilistic Language Model},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  pages = {1137--1155},
  year = {2003},
  month = {Feb},
}
@inproceedings{NIPS2017_3f5ee243,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit,
            Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and
            Polosukhin, Illia},
  booktitle = {Advances in Neural Information Processing Systems},
  editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R.
            Fergus and S. Vishwanathan and R. Garnett},
  pages = {},
  publisher = {Curran Associates, Inc.},
  title = {Attention is All you Need},
  url = {
         https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
         },
  volume = {30},
  year = {2017},
}
@inproceedings{SparckJones1975ReportOT,
  title = {Report on the need for and provision of an 'ideal' information
           retrieval test collection},
  author = {Karen Sparck Jones and C. J. van Rijsbergen},
  year = {1975},
  url = {https://api.semanticscholar.org/CorpusID:60988681},
}
