\documentclass{article}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage[margin=1in]{geometry}

\usepackage[backend=biber, style=numeric, sorting=nty]{biblatex}
\addbibresource{../references.bib}

\input{../../code/tbls/character_stats.tex}
\input{../../code/tbls/word_stats.tex}
\input{../../code/tbls/general_stats.tex}
\input{../../code/tbls/similarities_stats.tex}

\newcommand{\titlecite}[1]{%
  \emph{\parencite{#1} \citetitle{#1}}%
}

% \usepackage{siunitx}
% \sisetup{
%   output-decimal-marker = {,},
%   group-separator = {.}
% }

\title{Research proposal: Detecting fraud in language with Generative Retrieval models. An Enron Corpus study.}
\author{DaniÃ«l van Oosteroom, \\
  \small Supervisor: Yubao Tang
}

\date{\today}
% Introcuctie 
% Bachground 
% - related research
% - toepassings domijn data research
% Methods
% experimental setup - Data analysis
% results
% Discussion
% Conclusions 
\begin{document}


\maketitle

% \begin{multicols}{2}

\section{Introduction}


To what extend can GR be applied to detect fraudulent language markers in the Enron Corpus?
(intent detection or emotion detection)

1. RQ1: How does Generative Retrieval outperform BM25 and dense vector embeddings on general document retrieval within the Enron Corpus. \\
2. RQ2: How effectively does GR retrieve fraudulent documents in the Enron Corpus. \\
\\
- TODO: write introduction

introduce task definition 
task barkground and limitation that I want to adress. And key idea of your method
and results

\section{Related Research}

(add pshycholinguistic references)
To Fraud detection is a growing field. In particular, fraud detection on textual unstructured data is novel and unsolved \cite{blanco2020manifestation, noever2020enroncorpusemailbodies}. Moreover, no trustworthy, peer reviewed research has been published about this. It could be possible to detect fraud from text because perpetrators might leave behind linguistic markers or show emotions that are common with fraud.

To be able to do research on this it is imperative that there is a large corpus which contains fraud. This is why the Enron corpus is such an addition to the field \cite{klimt2004introducing,klimt2004enron, enron_dataset}. Enron was energy company that specialized in oil and gas and filed for bankruptcy in 2001 \cite{BritannicaEnron, FBIEnronCase}. During the investigation this dataset was made public and a cleaned version is publicly available \cite{enron_dataset}. 

Could it be possible to retrieve fraudulent documents. The field of information retrieval is old and has had many different systems to achieve this goal. One that has been dominant for a while is BM25 \cite{INR-019}. This uses in part the inverted term frequency index to search for documents. This is limited because of the semantic gap. This is where certain tokens in the document that are important for meaning do not show up in the query even though they are similar. Newer but computationally heavier methods include dense vector embedding models \cite{bengio2003neural} solve this problem by translating query and documents to a vector representation and matching documents to a query by taking the cosine distance between their respective vectors. 

%TODO: move embedding models down to paragraph below

Recently there has been a paradigm shift in NLP processing with the introduction of Transformers \cite{NIPS2017_3f5ee243}. This is a version of a recurrent neural network that encodes not only tokens but also their position. This done called attention. Where before language models were build by linguists, mapping words to synonyms or related words now a neural network can be trained on really large corpus and learn this behaviour unsupervised. Because of the size these are commonly refered to as Large Language Models (LLMs).

This opens up the question. Could this be used for retrieving documents? This where the field of Generative Retrieval Models comes in (GR). The advantage that generative 
%TODO: 1. extend literature review
%TODO: 2. make it more academic like (no emotions and formal writing like French/Latin word and not Germanic)
TODO: talk about pooling \cite{SparckJones1975ReportOT}, T5 \cite{JMLR:v21:20-074}, DSG \cite{NEURIPS2022_892840a6} and Elaborative Descriptions \cite{10.1145/3580305.3599903}
\\
\\
- TODO: Talk about the Enron data and what other research have found out about it while analyzing it.

Task definition
fraud detection (also do intention detection and sentiment analysis)
relevant research
What will I do

generative retrieval 
what it is 
relevant research
what I will do 

How they work on this
key idea of the method.

Do research on fraud detection.
Introduce task and give overview
Then what makes yours unique.


details in the methods

\section{Methods}

\subsection{Task Definition}
\label{subsec:task-definition}

Let $\mathcal{D} = \{d_1, d_2, \dots d_N \}$ denote the corpus of N email documents, which consists of subject, sender, recipient and sender. Let $\mathcal{Q} = \{q_1, q_2, \dots q_M \}$ represent the set of M queries. Further, we define $\mathcal{Y} = \{y_1, y_2, \dots y_N \}$ as the set of unique document identifiers corresponding to each email document. \\
Let $f_\theta$ be the GR model parameterized by $\theta$. The task of the model is to map query $q \in \mathcal{Q}$ or $d \in \mathcal{D}$ to output relevant documents identifier $y \in \mathcal{Y}$.

To do this, a training set is construction $\mathcal{T}$, this is defined as the union of the document-id pair and query-id pair~\eqref{eq:training-set}.
\begin{equation}
  \mathcal{T} = \{(d,y_d) | d \in \mathcal{D} \} \cup \{(q, y_q) | q \in \mathcal{Q}\}
  \label{eq:training-set}
\end{equation}
Where $y_d \in \mathcal{Y}$ is the unique identifier assigned to document d and $y_q \in \mathcal{Q}$ is the unique identifier assigned to document q.

The GR retrieval model, $f_\theta$ follows an encoder-decoder architecture. The encoder encodes the query q or the email document d and generates the unique document identifier y.

\subsection{Dataset}
The datalanalys is done before truncating the emails.
The number of emails present after dropping automatically generated folders is \NoFolderLength. This include the inbox folders and sent / sent\_items / outbox. This is a similar number to the cleaned dataset found in \citeauthor{klimt2004enron}. They found a value of 200,399 documents after cleaning. Their different number of selected emails may have been due to theirobjective to classify the folders that each email belonged to, for which reason they sought only folders created by humans. 

After cleaning further the number of remaining emails left is \GeneralLength. 

To clean the bodies of the emails, it is imperative to remove the extra syntax that email providers like outlook enter automatically. Also messages smaller then 20 words are removed as they are to small to create queries and docids of.

\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.48\textwidth]{../../code/plots/character_count_kde.png}
        \label{fig:chr_count_kde}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.48\textwidth]{../../code/plots/word_count_kde.png}
        \label{fig:wrd_count_kde}
    }%
    \caption{KDE plot}
    \label{fig:main_kde_plots}
\end{figure*}


\begin{figure*}[ht!]
    \centering
    \subfloat[Character Count]{
        \includegraphics[width=0.48\textwidth]{../../code/plots/character_counts_boxplot.png}
        \label{fig:chr_box_plot}
    }%
    \subfloat[Word Count]{
        \includegraphics[width=0.48\textwidth]{../../code/plots/word_counts_boxplot.png}
        \label{fig:wrd_box_plot}
    }%
    \caption{Box plot}
    \label{fig:main_box_plots}
\end{figure*}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.8\textwidth]{../../code/plots/similarities_hist.png}
  \caption{Distribution of similarities}
  \label{fig:similarities}
\end{figure*}

% TODO: only use Q1 and Q3. Get too many too long documents out. Beginning is most interesting so truncating would be fine (happens automatically). Case study biggest email. \\
% TODO: Do distribution of folder per 
% TODO: 10 thoudand and 100. Slice of all the users. 
% TODO: T-SNE

For the word count the Mean \wordMean{} as shown in \ref{fig:wrd_count_kde} and \ref{fig:wrd_box_plot} is substantially larger than the Median \wordMedian{} which indicates that this is not a normal distribution. This becomes clearer when comparing the Q3 \wordQthree{} to the Max value \wordMax{}. The substantial gap explains the size of the mean. The same is applicable to the character count.  (TODO: add the skewness of the )

To determine if the subjects of the emails have any similarity to their respective bodies a sentence transformer will be used to calculate the embedding and the cosine similarity will be computed. In the process of calculating the cosine similarity the dot product of the subject and body embeddings is taken and the distribution is plotted in figure \ref{fig:similarities}. Although the distribution is unimodal, it is not a perfect bell curve. This is also apparent when looking at the Skewness which is a negative skew of \similaritiesSkewness. It is clear that the subjects and the bodies are very similar. \\

The training of the model was conducted in two stages. In the first stage, a set containing 10,000 emails was curated to verify if the data was suitable for the current objective and that the experimental setup contained no errors. When it was confirmed that the experimental setup was successful, a 100,000-email dataset was curated for the final training. \textcite{NEURIPS2022_892840a6} uses different sizes in datasets to give better insight into the performance of a retrieval methods, because different systems perform better at different size datasets. Due to limited emails contained in this dataset the 320.000-email dataset mentioned in the original DSI paper was not created in this experiment. In all curated datasets the ratio of emails per user-folder was maintained.
\\\\
- TODO:
  \begin{itemize}
    \item analyze subject body similarity (how do I decide what model to use?). Similarity, Sentence transformer T5 \cite{ni-etal-2022-sentence}
    \item distribute folder size per user (bar plot automatically generated size and not automatically generated (maybe top 10))  
      \begin{itemize}
        \item how do to manage the amount of different folders in displaying
        \item add count distinct folder names
        \item folder name topics
      \end{itemize}
    \item analyze distribution of folder names.
    \item Cluster / topic modeling with LDA to reveal major themes
    \item Plot a temporal distribution of the emails
    \item figure out how at what size emails will be truncated and truncate them. 
  \end{itemize}


\subsection{Query Construction}

TODO: \\
  \begin{itemize}
    \item make a 10k and 100k dataset that equally uses from all users and folders. By random data as well and size. 
    \item generate elaborative descriptions
    \item generate queries (13k-100k for 100k) (half t5 query generation)
    \item do I need to generate separate test queries for evaluation? (extract 1000 to 5000 for the test) 
    \item decide on the use of the following techniques
      \begin{itemize}
        \item text rank\\ 
          Text rank is great for ED because it is quick and cheap and keeps the core of the meaning intact. In making the ED, add in the subject. The sentences use for the ED are 1/3 of the sentences + subject that are in the email with a max of 3. Experiment if you dont get any duplicates
        \item **Will not use doc2query/docTTTTTquery \\
        \item put these into bibtex for reference
      \end{itemize}
    \item use document clustering for generating queries?
  \end{itemize}

\subsection{Model training}
Question: do I comment on the improvements between the original DSI paper \cite{NEURIPS2022_892840a6} and your improvement \cite{10.1145/3580305.3599903} or can I just quote that?
As specified in \cref{subsec:task-definition} $f_\theta$ is a GR model with the encoder decoder architecture. For this the T5 model was chosen because that is the standard used in previous works \cite{NEURIPS2022_892840a6}\cite{10.1145/3580305.3599903}. The training of the model is a joint mode of two parts i.e., indexing and the retrieval in an end to end way.\\
\textbf{Indexing:} This step is for memorizing information about the email documents, \textcite{NEURIPS2022_892840a6} takes a document and maps it to an unique document identifier.

\begin{equation}
  \mathcal{L}_{Indexing}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:indexing-los}
\end{equation}

While \textcite{NEURIPS2022_892840a6} use only numerical ids \textcite{10.1145/3580305.3599903} use Elaborative Descriptions as the identifier. 

\textbf{Retrieval:} Given a query $q \in \mathcal{Q}$, the DSI model outputs the relevant document identifier y. For this the T5 model, previously fine tuned for indexing, is trained on the query-id pair.

\begin{equation}
  \mathcal{L}_{Retrieval}{(\theta)} = \displaystyle\sum_{d_i \in \mathcal{D}} log P(y | T5_{\theta}(d_i))
  \label{eq:retrieval-loss}
\end{equation}


TODO: 
  \begin{itemize}
    \item add more information about the combination trainging and the beam search method
  \end{itemize}

\subsection{Baseline Methods}

BM25 vector embedding

\subsection{Evaluation metrics}


\subsection{sentiment Analysis}
TODO: 
  \begin{itemize}
    \item Sentiment analysis on emotions like fear, anger, trust, anticipation, fear, love, sadness and surprise and analyze their relation ship to the convicted people. \cite{noever2020enroncorpusemailbodies}
  \end{itemize}

\subsection{Reproducing research linguistic markers}
TOO: 
Use the linguistic Markers datasets to analyze which emails and users contain the most \cite{blanco2020manifestation} % TODO: refer to the direct datasets


\subsection{Query Construction}
TODO:
Create queries depending on the Linguistic markers dataset \cite{blanco2020manifestation} % TODO: refer to the direct datasets

\subsection{Fraud Evaluation}
TODO:
- Compare what was found to results found in \titlecite{blanco2020manifestation} and \titlecite{noever2020enroncorpusemailbodies}
- see if the found documents which are of the users that have committed fraud. This can be checked to which people have been persecuted (find source for this here \cite{FBIEnronCase})

\section{Experimental setup}

\subsection{Evaluation metrics}


\section{Results}
-
\section{Conclusion}
-
\section{Discussion}
-
\printbibliography

% \end{multicols}

\end{document}
